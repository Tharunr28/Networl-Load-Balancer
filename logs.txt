
==> Audit <==
|---------|--------------------|----------|----------------------|---------|---------------------|---------------------|
| Command |        Args        | Profile  |         User         | Version |     Start Time      |      End Time       |
|---------|--------------------|----------|----------------------|---------|---------------------|---------------------|
| start   |                    | minikube | DESKTOP-9QR03OR\DELL | v1.34.0 | 15 Sep 24 15:01 IST |                     |
| start   |                    | minikube | DESKTOP-9QR03OR\DELL | v1.34.0 | 15 Sep 24 16:09 IST |                     |
| start   |                    | minikube | DESKTOP-9QR03OR\DELL | v1.34.0 | 15 Sep 24 16:34 IST |                     |
| start   |                    | minikube | DESKTOP-9QR03OR\DELL | v1.34.0 | 15 Sep 24 16:44 IST |                     |
| delete  |                    | minikube | DESKTOP-9QR03OR\DELL | v1.34.0 | 15 Sep 24 16:44 IST | 15 Sep 24 16:44 IST |
| start   |                    | minikube | DESKTOP-9QR03OR\DELL | v1.34.0 | 15 Sep 24 16:44 IST | 15 Sep 24 16:50 IST |
| start   |                    | minikube | DESKTOP-9QR03OR\DELL | v1.34.0 | 16 Sep 24 01:44 IST | 16 Sep 24 01:45 IST |
| start   |                    | minikube | DESKTOP-9QR03OR\DELL | v1.34.0 | 16 Sep 24 10:55 IST |                     |
| start   |                    | minikube | DESKTOP-9QR03OR\DELL | v1.34.0 | 16 Sep 24 11:07 IST |                     |
| start   |                    | minikube | DESKTOP-9QR03OR\DELL | v1.34.0 | 16 Sep 24 11:11 IST |                     |
| start   |                    | minikube | DESKTOP-9QR03OR\DELL | v1.34.0 | 16 Sep 24 11:19 IST |                     |
| stop    |                    | minikube | DESKTOP-9QR03OR\DELL | v1.34.0 | 16 Sep 24 11:26 IST | 16 Sep 24 11:26 IST |
| start   |                    | minikube | DESKTOP-9QR03OR\DELL | v1.34.0 | 16 Sep 24 11:26 IST |                     |
| stop    |                    | minikube | DESKTOP-9QR03OR\DELL | v1.34.0 | 16 Sep 24 11:27 IST | 16 Sep 24 11:27 IST |
| start   |                    | minikube | DESKTOP-9QR03OR\DELL | v1.34.0 | 16 Sep 24 11:27 IST |                     |
| stop    |                    | minikube | DESKTOP-9QR03OR\DELL | v1.34.0 | 16 Sep 24 11:30 IST | 16 Sep 24 11:30 IST |
| start   |                    | minikube | DESKTOP-9QR03OR\DELL | v1.34.0 | 16 Sep 24 11:30 IST |                     |
| start   |                    | minikube | DESKTOP-9QR03OR\DELL | v1.34.0 | 16 Sep 24 11:38 IST |                     |
| stop    |                    | minikube | DESKTOP-9QR03OR\DELL | v1.34.0 | 16 Sep 24 12:11 IST | 16 Sep 24 12:11 IST |
| start   |                    | minikube | DESKTOP-9QR03OR\DELL | v1.34.0 | 16 Sep 24 12:11 IST |                     |
| start   |                    | minikube | DESKTOP-9QR03OR\DELL | v1.34.0 | 16 Sep 24 13:45 IST |                     |
| start   |                    | minikube | DESKTOP-9QR03OR\DELL | v1.34.0 | 16 Sep 24 13:53 IST |                     |
| start   |                    | minikube | DESKTOP-9QR03OR\DELL | v1.34.0 | 16 Sep 24 13:55 IST | 16 Sep 24 13:56 IST |
| service | flask-test-service | minikube | DESKTOP-9QR03OR\DELL | v1.34.0 | 16 Sep 24 13:59 IST |                     |
| service | flask-test-service | minikube | DESKTOP-9QR03OR\DELL | v1.34.0 | 16 Sep 24 14:20 IST |                     |
| service | flask-test-service | minikube | DESKTOP-9QR03OR\DELL | v1.34.0 | 16 Sep 24 14:30 IST |                     |
| service | flask-test-service | minikube | DESKTOP-9QR03OR\DELL | v1.34.0 | 16 Sep 24 15:43 IST | 16 Sep 24 17:14 IST |
| service | flask-test-service | minikube | DESKTOP-9QR03OR\DELL | v1.34.0 | 16 Sep 24 17:14 IST | 16 Sep 24 17:51 IST |
| service | flask-test-service | minikube | DESKTOP-9QR03OR\DELL | v1.34.0 | 16 Sep 24 18:09 IST | 16 Sep 24 18:11 IST |
| service | flask-test-service | minikube | DESKTOP-9QR03OR\DELL | v1.34.0 | 16 Sep 24 18:11 IST | 16 Sep 24 20:03 IST |
| service | flask-test-service | minikube | DESKTOP-9QR03OR\DELL | v1.34.0 | 16 Sep 24 20:03 IST |                     |
| stop    |                    | minikube | DESKTOP-9QR03OR\DELL | v1.34.0 | 17 Sep 24 00:05 IST |                     |
| start   |                    | minikube | DESKTOP-9QR03OR\DELL | v1.34.0 | 17 Sep 24 09:40 IST | 17 Sep 24 09:41 IST |
| service | flask-test-service | minikube | DESKTOP-9QR03OR\DELL | v1.34.0 | 17 Sep 24 09:41 IST | 17 Sep 24 09:59 IST |
| start   |                    | minikube | DESKTOP-9QR03OR\DELL | v1.34.0 | 17 Sep 24 09:55 IST |                     |
| stop    |                    | minikube | DESKTOP-9QR03OR\DELL | v1.34.0 | 17 Sep 24 09:58 IST | 17 Sep 24 09:58 IST |
| start   |                    | minikube | DESKTOP-9QR03OR\DELL | v1.34.0 | 17 Sep 24 09:59 IST |                     |
| start   |                    | minikube | DESKTOP-9QR03OR\DELL | v1.34.0 | 17 Sep 24 10:02 IST |                     |
| start   |                    | minikube | DESKTOP-9QR03OR\DELL | v1.34.0 | 17 Sep 24 10:07 IST |                     |
| stop    |                    | minikube | DESKTOP-9QR03OR\DELL | v1.34.0 | 17 Sep 24 10:10 IST | 17 Sep 24 10:11 IST |
| start   |                    | minikube | DESKTOP-9QR03OR\DELL | v1.34.0 | 17 Sep 24 10:12 IST |                     |
| start   |                    | minikube | DESKTOP-9QR03OR\DELL | v1.34.0 | 17 Sep 24 10:17 IST | 17 Sep 24 10:19 IST |
| service | flask-test-service | minikube | DESKTOP-9QR03OR\DELL | v1.34.0 | 17 Sep 24 10:24 IST |                     |
| service | flask-test-service | minikube | DESKTOP-9QR03OR\DELL | v1.34.0 | 17 Sep 24 10:25 IST |                     |
|---------|--------------------|----------|----------------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2024/09/17 10:17:42
Running on machine: DESKTOP-9QR03OR
Binary: Built with gc go1.22.5 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0917 10:17:42.914627   16648 out.go:345] Setting OutFile to fd 100 ...
I0917 10:17:42.916488   16648 out.go:358] Setting ErrFile to fd 104...
I0917 10:17:42.964975   16648 out.go:352] Setting JSON to false
I0917 10:17:42.972141   16648 start.go:129] hostinfo: {"hostname":"DESKTOP-9QR03OR","uptime":173,"bootTime":1726548289,"procs":231,"os":"windows","platform":"Microsoft Windows 11 Home Single Language","platformFamily":"Standalone Workstation","platformVersion":"10.0.22631.4169 Build 22631.4169","kernelVersion":"10.0.22631.4169 Build 22631.4169","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"fc96b21b-b7ab-4297-9976-8f26788b2914"}
W0917 10:17:42.972709   16648 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0917 10:17:42.974337   16648 out.go:177] ðŸ˜„  minikube v1.34.0 on Microsoft Windows 11 Home Single Language 10.0.22631.4169 Build 22631.4169
I0917 10:17:42.975990   16648 notify.go:220] Checking for updates...
I0917 10:17:42.996367   16648 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I0917 10:17:42.998882   16648 driver.go:394] Setting default libvirt URI to qemu:///system
I0917 10:17:43.299502   16648 docker.go:123] docker version: linux-27.2.0:Docker Desktop 4.34.2 (167172)
I0917 10:17:43.308440   16648 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0917 10:17:45.493978   16648 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (2.1855387s)
I0917 10:17:45.497241   16648 info.go:266] docker info: {ID:94debfe0-3a1f-4996-af54-b5221365fa03 Containers:21 ContainersRunning:18 ContainersPaused:0 ContainersStopped:3 Images:9 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:60 OomKillDisable:true NGoroutines:108 SystemTime:2024-09-17 04:47:45.47616018 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:15 KernelVersion:5.15.153.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:4035743744 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:27.2.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:8fc6bcff51318944179630522a095cc9dbf9f353 Expected:8fc6bcff51318944179630522a095cc9dbf9f353} RuncCommit:{ID:v1.1.13-0-g58aa920 Expected:v1.1.13-0-g58aa920} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.16.2-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.29.2-desktop.2] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.34] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Alpha) Vendor:Docker Inc. Version:v0.0.15] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.25] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.3.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.13.0]] Warnings:<nil>}}
I0917 10:17:45.499414   16648 out.go:177] âœ¨  Using the docker driver based on existing profile
I0917 10:17:45.501132   16648 start.go:297] selected driver: docker
I0917 10:17:45.501132   16648 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\DELL:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0917 10:17:45.501132   16648 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0917 10:17:45.518352   16648 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0917 10:17:45.845691   16648 info.go:266] docker info: {ID:94debfe0-3a1f-4996-af54-b5221365fa03 Containers:21 ContainersRunning:18 ContainersPaused:0 ContainersStopped:3 Images:9 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:60 OomKillDisable:true NGoroutines:108 SystemTime:2024-09-17 04:47:45.832739212 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:15 KernelVersion:5.15.153.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:4035743744 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:27.2.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:8fc6bcff51318944179630522a095cc9dbf9f353 Expected:8fc6bcff51318944179630522a095cc9dbf9f353} RuncCommit:{ID:v1.1.13-0-g58aa920 Expected:v1.1.13-0-g58aa920} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.16.2-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.29.2-desktop.2] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.34] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Alpha) Vendor:Docker Inc. Version:v0.0.15] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.25] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.3.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.13.0]] Warnings:<nil>}}
I0917 10:17:45.902784   16648 cni.go:84] Creating CNI manager for ""
I0917 10:17:45.902784   16648 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0917 10:17:45.903331   16648 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\DELL:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0917 10:17:45.904968   16648 out.go:177] ðŸ‘  Starting "minikube" primary control-plane node in "minikube" cluster
I0917 10:17:45.906638   16648 cache.go:121] Beginning downloading kic base image for docker with docker
I0917 10:17:45.907729   16648 out.go:177] ðŸšœ  Pulling base image v0.0.45 ...
I0917 10:17:45.910018   16648 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I0917 10:17:45.910572   16648 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local docker daemon
I0917 10:17:45.910572   16648 preload.go:146] Found local preload: C:\Users\DELL\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4
I0917 10:17:45.910572   16648 cache.go:56] Caching tarball of preloaded images
I0917 10:17:45.911117   16648 preload.go:172] Found C:\Users\DELL\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0917 10:17:45.911652   16648 cache.go:59] Finished verifying existence of preloaded tar for v1.31.0 on docker
I0917 10:17:45.911652   16648 profile.go:143] Saving config to C:\Users\DELL\.minikube\profiles\minikube\config.json ...
I0917 10:17:45.993019   16648 cache.go:149] Downloading gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 to local cache
I0917 10:17:45.994660   16648 localpath.go:151] windows sanitize: C:\Users\DELL\.minikube\cache\kic\amd64\kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar -> C:\Users\DELL\.minikube\cache\kic\amd64\kicbase_v0.0.45@sha256_81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar
I0917 10:17:45.994660   16648 localpath.go:151] windows sanitize: C:\Users\DELL\.minikube\cache\kic\amd64\kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar -> C:\Users\DELL\.minikube\cache\kic\amd64\kicbase_v0.0.45@sha256_81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar
I0917 10:17:45.994660   16648 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory
I0917 10:17:45.995750   16648 image.go:66] Found gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory, skipping pull
I0917 10:17:45.995750   16648 image.go:135] gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 exists in cache, skipping pull
I0917 10:17:45.995750   16648 cache.go:152] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 as a tarball
I0917 10:17:45.995750   16648 cache.go:162] Loading gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from local cache
I0917 10:17:45.995750   16648 localpath.go:151] windows sanitize: C:\Users\DELL\.minikube\cache\kic\amd64\kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar -> C:\Users\DELL\.minikube\cache\kic\amd64\kicbase_v0.0.45@sha256_81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar
I0917 10:18:26.562933   16648 cache.go:164] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from cached tarball
I0917 10:18:26.563509   16648 cache.go:194] Successfully downloaded all kic artifacts
I0917 10:18:26.565487   16648 start.go:360] acquireMachinesLock for minikube: {Name:mkec786be7b8f8d7d72f4c3ca5ce7520987e23b4 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0917 10:18:26.566057   16648 start.go:364] duration metric: took 570.7Âµs to acquireMachinesLock for "minikube"
I0917 10:18:26.566057   16648 start.go:96] Skipping create...Using existing machine configuration
I0917 10:18:26.566618   16648 fix.go:54] fixHost starting: 
I0917 10:18:26.578949   16648 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0917 10:18:26.654763   16648 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0917 10:18:26.655406   16648 fix.go:112] recreateIfNeeded on minikube: state= err=unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0917 10:18:26.655406   16648 fix.go:117] machineExists: false. err=machine does not exist
I0917 10:18:26.657287   16648 out.go:177] ðŸ¤·  docker "minikube" container is missing, will recreate.
I0917 10:18:26.658908   16648 delete.go:124] DEMOLISHING minikube ...
I0917 10:18:26.670244   16648 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0917 10:18:26.733818   16648 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
W0917 10:18:26.734329   16648 stop.go:83] unable to get state: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0917 10:18:26.734329   16648 delete.go:128] stophost failed (probably ok): ssh power off: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0917 10:18:26.747733   16648 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0917 10:18:26.810914   16648 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0917 10:18:26.811444   16648 delete.go:82] Unable to get host status for minikube, assuming it has already been deleted: state: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0917 10:18:26.818059   16648 cli_runner.go:164] Run: docker container inspect -f {{.Id}} minikube
W0917 10:18:26.886278   16648 cli_runner.go:211] docker container inspect -f {{.Id}} minikube returned with exit code 1
I0917 10:18:26.886789   16648 kic.go:371] could not find the container minikube to remove it. will try anyways
I0917 10:18:26.892866   16648 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0917 10:18:26.961971   16648 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
W0917 10:18:26.962623   16648 oci.go:84] error getting container status, will try to delete anyways: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0917 10:18:26.967499   16648 cli_runner.go:164] Run: docker exec --privileged -t minikube /bin/bash -c "sudo init 0"
W0917 10:18:27.034051   16648 cli_runner.go:211] docker exec --privileged -t minikube /bin/bash -c "sudo init 0" returned with exit code 1
I0917 10:18:27.034051   16648 oci.go:650] error shutdown minikube: docker exec --privileged -t minikube /bin/bash -c "sudo init 0": exit status 1
stdout:

stderr:
Error response from daemon: No such container: minikube
I0917 10:18:28.047382   16648 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0917 10:18:28.140330   16648 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0917 10:18:28.140330   16648 oci.go:662] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0917 10:18:28.140330   16648 oci.go:664] temporary error: container minikube status is  but expect it to be exited
I0917 10:18:28.140840   16648 retry.go:31] will retry after 721.245771ms: couldn't verify container is exited. %v: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0917 10:18:28.881649   16648 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0917 10:18:28.949505   16648 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0917 10:18:28.949505   16648 oci.go:662] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0917 10:18:28.949505   16648 oci.go:664] temporary error: container minikube status is  but expect it to be exited
I0917 10:18:28.949505   16648 retry.go:31] will retry after 664.980008ms: couldn't verify container is exited. %v: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0917 10:18:29.692612   16648 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0917 10:18:29.813242   16648 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0917 10:18:29.813242   16648 oci.go:662] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0917 10:18:29.813242   16648 oci.go:664] temporary error: container minikube status is  but expect it to be exited
I0917 10:18:29.813242   16648 retry.go:31] will retry after 836.866664ms: couldn't verify container is exited. %v: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0917 10:18:30.667510   16648 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0917 10:18:30.751868   16648 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0917 10:18:30.751868   16648 oci.go:662] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0917 10:18:30.751868   16648 oci.go:664] temporary error: container minikube status is  but expect it to be exited
I0917 10:18:30.751868   16648 retry.go:31] will retry after 2.198396324s: couldn't verify container is exited. %v: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0917 10:18:32.965984   16648 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0917 10:18:33.043962   16648 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0917 10:18:33.043962   16648 oci.go:662] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0917 10:18:33.043962   16648 oci.go:664] temporary error: container minikube status is  but expect it to be exited
I0917 10:18:33.043962   16648 retry.go:31] will retry after 2.053789231s: couldn't verify container is exited. %v: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0917 10:18:35.111733   16648 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0917 10:18:35.177950   16648 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0917 10:18:35.177950   16648 oci.go:662] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0917 10:18:35.177950   16648 oci.go:664] temporary error: container minikube status is  but expect it to be exited
I0917 10:18:35.178492   16648 retry.go:31] will retry after 3.779267045s: couldn't verify container is exited. %v: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0917 10:18:38.972669   16648 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0917 10:18:39.064622   16648 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0917 10:18:39.064622   16648 oci.go:662] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0917 10:18:39.064622   16648 oci.go:664] temporary error: container minikube status is  but expect it to be exited
I0917 10:18:39.064622   16648 retry.go:31] will retry after 5.624929462s: couldn't verify container is exited. %v: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0917 10:18:44.694718   16648 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0917 10:18:44.759125   16648 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0917 10:18:44.759125   16648 oci.go:662] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0917 10:18:44.759125   16648 oci.go:664] temporary error: container minikube status is  but expect it to be exited
I0917 10:18:44.759125   16648 oci.go:88] couldn't shut down minikube (might be okay): verify shutdown: couldn't verify container is exited. %v: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
 
I0917 10:18:44.765763   16648 cli_runner.go:164] Run: docker rm -f -v minikube
I0917 10:18:44.834193   16648 cli_runner.go:164] Run: docker container inspect -f {{.Id}} minikube
W0917 10:18:44.891469   16648 cli_runner.go:211] docker container inspect -f {{.Id}} minikube returned with exit code 1
I0917 10:18:44.897515   16648 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0917 10:18:44.970721   16648 cli_runner.go:164] Run: docker network rm minikube
I0917 10:18:45.237770   16648 fix.go:124] Sleeping 1 second for extra luck!
I0917 10:18:46.248939   16648 start.go:125] createHost starting for "" (driver="docker")
I0917 10:18:46.259726   16648 out.go:235] ðŸ”¥  Creating docker container (CPUs=2, Memory=2200MB) ...
I0917 10:18:46.261385   16648 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0917 10:18:46.261385   16648 client.go:168] LocalClient.Create starting
I0917 10:18:46.262688   16648 main.go:141] libmachine: Reading certificate data from C:\Users\DELL\.minikube\certs\ca.pem
I0917 10:18:46.277184   16648 main.go:141] libmachine: Decoding PEM data...
I0917 10:18:46.278260   16648 main.go:141] libmachine: Parsing certificate...
I0917 10:18:46.280493   16648 main.go:141] libmachine: Reading certificate data from C:\Users\DELL\.minikube\certs\cert.pem
I0917 10:18:46.315831   16648 main.go:141] libmachine: Decoding PEM data...
I0917 10:18:46.315831   16648 main.go:141] libmachine: Parsing certificate...
I0917 10:18:46.322667   16648 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0917 10:18:46.393241   16648 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0917 10:18:46.403353   16648 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...
I0917 10:18:46.403353   16648 cli_runner.go:164] Run: docker network inspect minikube
W0917 10:18:46.474512   16648 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0917 10:18:46.474512   16648 network_create.go:287] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0917 10:18:46.474512   16648 network_create.go:289] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0917 10:18:46.482450   16648 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0917 10:18:46.626574   16648 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc000b0af00}
I0917 10:18:46.627104   16648 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I0917 10:18:46.632720   16648 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0917 10:18:46.788406   16648 network_create.go:108] docker network minikube 192.168.49.0/24 created
I0917 10:18:46.788938   16648 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I0917 10:18:46.800772   16648 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0917 10:18:46.887440   16648 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0917 10:18:46.964149   16648 oci.go:103] Successfully created a docker volume minikube
I0917 10:18:46.976008   16648 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 -d /var/lib
I0917 10:18:48.718239   16648 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 -d /var/lib: (1.7422312s)
I0917 10:18:48.718239   16648 oci.go:107] Successfully prepared a docker volume minikube
I0917 10:18:48.718239   16648 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I0917 10:18:48.718784   16648 kic.go:194] Starting extracting preloaded images to volume ...
I0917 10:18:48.733847   16648 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\DELL\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 -I lz4 -xf /preloaded.tar -C /extractDir
I0917 10:18:59.407600   16648 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\DELL\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 -I lz4 -xf /preloaded.tar -C /extractDir: (10.6732458s)
I0917 10:18:59.407600   16648 kic.go:203] duration metric: took 10.6893607s to extract preloaded images to volume ...
I0917 10:18:59.412624   16648 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0917 10:18:59.823280   16648 info.go:266] docker info: {ID:94debfe0-3a1f-4996-af54-b5221365fa03 Containers:21 ContainersRunning:18 ContainersPaused:0 ContainersStopped:3 Images:10 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:60 OomKillDisable:true NGoroutines:103 SystemTime:2024-09-17 04:48:59.799020469 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:15 KernelVersion:5.15.153.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:4035743744 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:27.2.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:8fc6bcff51318944179630522a095cc9dbf9f353 Expected:8fc6bcff51318944179630522a095cc9dbf9f353} RuncCommit:{ID:v1.1.13-0-g58aa920 Expected:v1.1.13-0-g58aa920} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.16.2-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.29.2-desktop.2] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.34] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Alpha) Vendor:Docker Inc. Version:v0.0.15] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.25] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.3.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.13.0]] Warnings:<nil>}}
I0917 10:18:59.828608   16648 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0917 10:19:00.193866   16648 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=2200mb --memory-swap=2200mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85
I0917 10:19:00.959231   16648 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0917 10:19:01.051434   16648 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0917 10:19:01.132495   16648 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0917 10:19:01.250610   16648 oci.go:144] the created container "minikube" has a running status.
I0917 10:19:01.250610   16648 kic.go:225] Creating ssh key for kic: C:\Users\DELL\.minikube\machines\minikube\id_rsa...
I0917 10:19:01.648721   16648 kic_runner.go:191] docker (temp): C:\Users\DELL\.minikube\machines\minikube\id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0917 10:19:01.748145   16648 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0917 10:19:01.833077   16648 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0917 10:19:01.833077   16648 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0917 10:19:01.957981   16648 kic.go:265] ensuring only current user has permissions to key file located at : C:\Users\DELL\.minikube\machines\minikube\id_rsa...
I0917 10:19:02.622851   16648 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0917 10:19:02.685379   16648 machine.go:93] provisionDockerMachine start ...
I0917 10:19:02.690184   16648 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 10:19:02.747634   16648 main.go:141] libmachine: Using SSH client type: native
I0917 10:19:02.760034   16648 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xc9c9c0] 0xc9f5a0 <nil>  [] 0s} 127.0.0.1 50875 <nil> <nil>}
I0917 10:19:02.760034   16648 main.go:141] libmachine: About to run SSH command:
hostname
I0917 10:19:02.967031   16648 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0917 10:19:02.967558   16648 ubuntu.go:169] provisioning hostname "minikube"
I0917 10:19:02.972892   16648 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 10:19:03.038831   16648 main.go:141] libmachine: Using SSH client type: native
I0917 10:19:03.038831   16648 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xc9c9c0] 0xc9f5a0 <nil>  [] 0s} 127.0.0.1 50875 <nil> <nil>}
I0917 10:19:03.038831   16648 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0917 10:19:03.214929   16648 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0917 10:19:03.220319   16648 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 10:19:03.284206   16648 main.go:141] libmachine: Using SSH client type: native
I0917 10:19:03.284722   16648 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xc9c9c0] 0xc9f5a0 <nil>  [] 0s} 127.0.0.1 50875 <nil> <nil>}
I0917 10:19:03.284722   16648 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0917 10:19:03.430155   16648 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0917 10:19:03.430155   16648 ubuntu.go:175] set auth options {CertDir:C:\Users\DELL\.minikube CaCertPath:C:\Users\DELL\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\DELL\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\DELL\.minikube\machines\server.pem ServerKeyPath:C:\Users\DELL\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\DELL\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\DELL\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\DELL\.minikube}
I0917 10:19:03.430155   16648 ubuntu.go:177] setting up certificates
I0917 10:19:03.430155   16648 provision.go:84] configureAuth start
I0917 10:19:03.436602   16648 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0917 10:19:03.499351   16648 provision.go:143] copyHostCerts
I0917 10:19:03.499351   16648 exec_runner.go:144] found C:\Users\DELL\.minikube/ca.pem, removing ...
I0917 10:19:03.499351   16648 exec_runner.go:203] rm: C:\Users\DELL\.minikube\ca.pem
I0917 10:19:03.499902   16648 exec_runner.go:151] cp: C:\Users\DELL\.minikube\certs\ca.pem --> C:\Users\DELL\.minikube/ca.pem (1070 bytes)
I0917 10:19:03.500958   16648 exec_runner.go:144] found C:\Users\DELL\.minikube/cert.pem, removing ...
I0917 10:19:03.500958   16648 exec_runner.go:203] rm: C:\Users\DELL\.minikube\cert.pem
I0917 10:19:03.500958   16648 exec_runner.go:151] cp: C:\Users\DELL\.minikube\certs\cert.pem --> C:\Users\DELL\.minikube/cert.pem (1115 bytes)
I0917 10:19:03.517790   16648 exec_runner.go:144] found C:\Users\DELL\.minikube/key.pem, removing ...
I0917 10:19:03.518303   16648 exec_runner.go:203] rm: C:\Users\DELL\.minikube\key.pem
I0917 10:19:03.518829   16648 exec_runner.go:151] cp: C:\Users\DELL\.minikube\certs\key.pem --> C:\Users\DELL\.minikube/key.pem (1679 bytes)
I0917 10:19:03.519895   16648 provision.go:117] generating server cert: C:\Users\DELL\.minikube\machines\server.pem ca-key=C:\Users\DELL\.minikube\certs\ca.pem private-key=C:\Users\DELL\.minikube\certs\ca-key.pem org=DELL.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0917 10:19:03.806428   16648 provision.go:177] copyRemoteCerts
I0917 10:19:03.819338   16648 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0917 10:19:03.825884   16648 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 10:19:03.889795   16648 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50875 SSHKeyPath:C:\Users\DELL\.minikube\machines\minikube\id_rsa Username:docker}
I0917 10:19:04.020520   16648 ssh_runner.go:362] scp C:\Users\DELL\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0917 10:19:04.086406   16648 ssh_runner.go:362] scp C:\Users\DELL\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1070 bytes)
I0917 10:19:04.130495   16648 ssh_runner.go:362] scp C:\Users\DELL\.minikube\machines\server.pem --> /etc/docker/server.pem (1172 bytes)
I0917 10:19:04.177213   16648 provision.go:87] duration metric: took 746.5244ms to configureAuth
I0917 10:19:04.177213   16648 ubuntu.go:193] setting minikube options for container-runtime
I0917 10:19:04.178294   16648 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I0917 10:19:04.184858   16648 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 10:19:04.269556   16648 main.go:141] libmachine: Using SSH client type: native
I0917 10:19:04.270159   16648 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xc9c9c0] 0xc9f5a0 <nil>  [] 0s} 127.0.0.1 50875 <nil> <nil>}
I0917 10:19:04.270159   16648 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0917 10:19:04.448771   16648 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0917 10:19:04.448771   16648 ubuntu.go:71] root file system type: overlay
I0917 10:19:04.449422   16648 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0917 10:19:04.456418   16648 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 10:19:04.589337   16648 main.go:141] libmachine: Using SSH client type: native
I0917 10:19:04.597581   16648 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xc9c9c0] 0xc9f5a0 <nil>  [] 0s} 127.0.0.1 50875 <nil> <nil>}
I0917 10:19:04.597581   16648 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0917 10:19:04.792156   16648 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0917 10:19:04.799889   16648 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 10:19:04.894494   16648 main.go:141] libmachine: Using SSH client type: native
I0917 10:19:04.894494   16648 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xc9c9c0] 0xc9f5a0 <nil>  [] 0s} 127.0.0.1 50875 <nil> <nil>}
I0917 10:19:04.894494   16648 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0917 10:19:06.527499   16648 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2024-08-27 14:13:43.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2024-09-17 04:49:04.781660117 +0000
@@ -1,46 +1,49 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0917 10:19:06.527499   16648 machine.go:96] duration metric: took 3.84212s to provisionDockerMachine
I0917 10:19:06.527499   16648 client.go:171] duration metric: took 20.2661142s to LocalClient.Create
I0917 10:19:06.527499   16648 start.go:167] duration metric: took 20.2666487s to libmachine.API.Create "minikube"
I0917 10:19:06.528048   16648 start.go:293] postStartSetup for "minikube" (driver="docker")
I0917 10:19:06.528048   16648 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0917 10:19:06.535585   16648 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0917 10:19:06.539452   16648 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 10:19:06.591428   16648 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50875 SSHKeyPath:C:\Users\DELL\.minikube\machines\minikube\id_rsa Username:docker}
I0917 10:19:06.712759   16648 ssh_runner.go:195] Run: cat /etc/os-release
I0917 10:19:06.718185   16648 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0917 10:19:06.718185   16648 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0917 10:19:06.718185   16648 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0917 10:19:06.718185   16648 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I0917 10:19:06.718185   16648 filesync.go:126] Scanning C:\Users\DELL\.minikube\addons for local assets ...
I0917 10:19:06.718185   16648 filesync.go:126] Scanning C:\Users\DELL\.minikube\files for local assets ...
I0917 10:19:06.718185   16648 start.go:296] duration metric: took 190.1371ms for postStartSetup
I0917 10:19:06.723721   16648 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0917 10:19:06.840836   16648 profile.go:143] Saving config to C:\Users\DELL\.minikube\profiles\minikube\config.json ...
I0917 10:19:06.877417   16648 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0917 10:19:06.881761   16648 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 10:19:06.944672   16648 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50875 SSHKeyPath:C:\Users\DELL\.minikube\machines\minikube\id_rsa Username:docker}
I0917 10:19:07.068806   16648 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0917 10:19:07.075074   16648 start.go:128] duration metric: took 20.8261352s to createHost
I0917 10:19:07.084784   16648 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0917 10:19:07.138564   16648 fix.go:138] unexpected machine state, will restart: <nil>
I0917 10:19:07.138564   16648 machine.go:93] provisionDockerMachine start ...
I0917 10:19:07.143279   16648 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 10:19:07.206734   16648 main.go:141] libmachine: Using SSH client type: native
I0917 10:19:07.207257   16648 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xc9c9c0] 0xc9f5a0 <nil>  [] 0s} 127.0.0.1 50875 <nil> <nil>}
I0917 10:19:07.207257   16648 main.go:141] libmachine: About to run SSH command:
hostname
I0917 10:19:07.349362   16648 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0917 10:19:07.349362   16648 ubuntu.go:169] provisioning hostname "minikube"
I0917 10:19:07.353769   16648 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 10:19:07.415314   16648 main.go:141] libmachine: Using SSH client type: native
I0917 10:19:07.415825   16648 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xc9c9c0] 0xc9f5a0 <nil>  [] 0s} 127.0.0.1 50875 <nil> <nil>}
I0917 10:19:07.415825   16648 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0917 10:19:07.583273   16648 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0917 10:19:07.589678   16648 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 10:19:07.649838   16648 main.go:141] libmachine: Using SSH client type: native
I0917 10:19:07.650444   16648 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xc9c9c0] 0xc9f5a0 <nil>  [] 0s} 127.0.0.1 50875 <nil> <nil>}
I0917 10:19:07.650444   16648 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0917 10:19:07.841625   16648 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0917 10:19:07.841625   16648 ubuntu.go:175] set auth options {CertDir:C:\Users\DELL\.minikube CaCertPath:C:\Users\DELL\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\DELL\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\DELL\.minikube\machines\server.pem ServerKeyPath:C:\Users\DELL\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\DELL\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\DELL\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\DELL\.minikube}
I0917 10:19:07.841625   16648 ubuntu.go:177] setting up certificates
I0917 10:19:07.841625   16648 provision.go:84] configureAuth start
I0917 10:19:07.847528   16648 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0917 10:19:07.907228   16648 provision.go:143] copyHostCerts
I0917 10:19:07.907826   16648 exec_runner.go:144] found C:\Users\DELL\.minikube/ca.pem, removing ...
I0917 10:19:07.907826   16648 exec_runner.go:203] rm: C:\Users\DELL\.minikube\ca.pem
I0917 10:19:07.908341   16648 exec_runner.go:151] cp: C:\Users\DELL\.minikube\certs\ca.pem --> C:\Users\DELL\.minikube/ca.pem (1070 bytes)
I0917 10:19:07.908871   16648 exec_runner.go:144] found C:\Users\DELL\.minikube/cert.pem, removing ...
I0917 10:19:07.908871   16648 exec_runner.go:203] rm: C:\Users\DELL\.minikube\cert.pem
I0917 10:19:07.908871   16648 exec_runner.go:151] cp: C:\Users\DELL\.minikube\certs\cert.pem --> C:\Users\DELL\.minikube/cert.pem (1115 bytes)
I0917 10:19:07.909967   16648 exec_runner.go:144] found C:\Users\DELL\.minikube/key.pem, removing ...
I0917 10:19:07.909967   16648 exec_runner.go:203] rm: C:\Users\DELL\.minikube\key.pem
I0917 10:19:07.909967   16648 exec_runner.go:151] cp: C:\Users\DELL\.minikube\certs\key.pem --> C:\Users\DELL\.minikube/key.pem (1679 bytes)
I0917 10:19:07.910515   16648 provision.go:117] generating server cert: C:\Users\DELL\.minikube\machines\server.pem ca-key=C:\Users\DELL\.minikube\certs\ca.pem private-key=C:\Users\DELL\.minikube\certs\ca-key.pem org=DELL.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0917 10:19:08.200134   16648 provision.go:177] copyRemoteCerts
I0917 10:19:08.214142   16648 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0917 10:19:08.220442   16648 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 10:19:08.337771   16648 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50875 SSHKeyPath:C:\Users\DELL\.minikube\machines\minikube\id_rsa Username:docker}
I0917 10:19:08.444838   16648 ssh_runner.go:362] scp C:\Users\DELL\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1070 bytes)
I0917 10:19:08.470300   16648 ssh_runner.go:362] scp C:\Users\DELL\.minikube\machines\server.pem --> /etc/docker/server.pem (1176 bytes)
I0917 10:19:08.500514   16648 ssh_runner.go:362] scp C:\Users\DELL\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0917 10:19:08.529221   16648 provision.go:87] duration metric: took 687.5963ms to configureAuth
I0917 10:19:08.529221   16648 ubuntu.go:193] setting minikube options for container-runtime
I0917 10:19:08.529761   16648 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I0917 10:19:08.533557   16648 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 10:19:08.595291   16648 main.go:141] libmachine: Using SSH client type: native
I0917 10:19:08.595892   16648 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xc9c9c0] 0xc9f5a0 <nil>  [] 0s} 127.0.0.1 50875 <nil> <nil>}
I0917 10:19:08.595892   16648 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0917 10:19:08.749908   16648 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0917 10:19:08.749908   16648 ubuntu.go:71] root file system type: overlay
I0917 10:19:08.750454   16648 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0917 10:19:08.755890   16648 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 10:19:08.814876   16648 main.go:141] libmachine: Using SSH client type: native
I0917 10:19:08.814876   16648 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xc9c9c0] 0xc9f5a0 <nil>  [] 0s} 127.0.0.1 50875 <nil> <nil>}
I0917 10:19:08.814876   16648 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0917 10:19:08.971020   16648 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0917 10:19:08.975378   16648 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 10:19:09.037283   16648 main.go:141] libmachine: Using SSH client type: native
I0917 10:19:09.037788   16648 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xc9c9c0] 0xc9f5a0 <nil>  [] 0s} 127.0.0.1 50875 <nil> <nil>}
I0917 10:19:09.037788   16648 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0917 10:19:09.250412   16648 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0917 10:19:09.250412   16648 machine.go:96] duration metric: took 2.1118477s to provisionDockerMachine
I0917 10:19:09.250412   16648 start.go:293] postStartSetup for "minikube" (driver="docker")
I0917 10:19:09.250412   16648 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0917 10:19:09.260842   16648 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0917 10:19:09.265150   16648 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 10:19:09.327032   16648 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50875 SSHKeyPath:C:\Users\DELL\.minikube\machines\minikube\id_rsa Username:docker}
I0917 10:19:09.443747   16648 ssh_runner.go:195] Run: cat /etc/os-release
I0917 10:19:09.450306   16648 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0917 10:19:09.450306   16648 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0917 10:19:09.450306   16648 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0917 10:19:09.450306   16648 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I0917 10:19:09.450306   16648 filesync.go:126] Scanning C:\Users\DELL\.minikube\addons for local assets ...
I0917 10:19:09.450833   16648 filesync.go:126] Scanning C:\Users\DELL\.minikube\files for local assets ...
I0917 10:19:09.450833   16648 start.go:296] duration metric: took 200.4212ms for postStartSetup
I0917 10:19:09.459925   16648 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0917 10:19:09.465752   16648 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 10:19:09.526437   16648 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50875 SSHKeyPath:C:\Users\DELL\.minikube\machines\minikube\id_rsa Username:docker}
I0917 10:19:09.642273   16648 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0917 10:19:09.648790   16648 fix.go:56] duration metric: took 43.0821721s for fixHost
I0917 10:19:09.649340   16648 start.go:83] releasing machines lock for "minikube", held for 43.0832828s
I0917 10:19:09.654731   16648 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0917 10:19:09.721797   16648 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I0917 10:19:09.728283   16648 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 10:19:09.729939   16648 ssh_runner.go:195] Run: cat /version.json
I0917 10:19:09.735660   16648 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 10:19:09.796067   16648 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50875 SSHKeyPath:C:\Users\DELL\.minikube\machines\minikube\id_rsa Username:docker}
I0917 10:19:09.800199   16648 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50875 SSHKeyPath:C:\Users\DELL\.minikube\machines\minikube\id_rsa Username:docker}
W0917 10:19:09.898864   16648 start.go:867] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I0917 10:19:09.906473   16648 ssh_runner.go:195] Run: systemctl --version
I0917 10:19:09.923806   16648 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0917 10:19:09.940326   16648 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0917 10:19:09.954237   16648 start.go:439] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0917 10:19:09.961735   16648 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0917 10:19:09.996797   16648 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist, /etc/cni/net.d/100-crio-bridge.conf] bridge cni config(s)
I0917 10:19:09.996797   16648 start.go:495] detecting cgroup driver to use...
I0917 10:19:09.996797   16648 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0917 10:19:09.997304   16648 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0917 10:19:10.023438   16648 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0917 10:19:10.044188   16648 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0917 10:19:10.057026   16648 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0917 10:19:10.064308   16648 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0917 10:19:10.087211   16648 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0917 10:19:10.108368   16648 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0917 10:19:10.127612   16648 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0917 10:19:10.148148   16648 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0917 10:19:10.168357   16648 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0917 10:19:10.189671   16648 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0917 10:19:10.257099   16648 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0917 10:19:10.279440   16648 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0917 10:19:10.298941   16648 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0917 10:19:10.319410   16648 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0917 10:19:10.441952   16648 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0917 10:19:10.593173   16648 start.go:495] detecting cgroup driver to use...
I0917 10:19:10.593173   16648 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0917 10:19:10.606338   16648 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0917 10:19:10.626272   16648 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0917 10:19:10.635545   16648 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0917 10:19:10.650687   16648 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0917 10:19:10.679802   16648 ssh_runner.go:195] Run: which cri-dockerd
I0917 10:19:10.691572   16648 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0917 10:19:10.702853   16648 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
W0917 10:19:10.734367   16648 out.go:270] â—  Failing to connect to https://registry.k8s.io/ from inside the minikube container
W0917 10:19:10.734898   16648 out.go:270] ðŸ’¡  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0917 10:19:10.741970   16648 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0917 10:19:10.906164   16648 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0917 10:19:11.014386   16648 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0917 10:19:11.014894   16648 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0917 10:19:11.049596   16648 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0917 10:19:11.204315   16648 ssh_runner.go:195] Run: sudo systemctl restart docker
I0917 10:19:11.767630   16648 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0917 10:19:11.796195   16648 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0917 10:19:11.820871   16648 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0917 10:19:11.930468   16648 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0917 10:19:12.076268   16648 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0917 10:19:12.239566   16648 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0917 10:19:12.383749   16648 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0917 10:19:12.409623   16648 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0917 10:19:12.525261   16648 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0917 10:19:12.643391   16648 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0917 10:19:12.654095   16648 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0917 10:19:12.660835   16648 start.go:563] Will wait 60s for crictl version
I0917 10:19:12.670529   16648 ssh_runner.go:195] Run: which crictl
I0917 10:19:12.683318   16648 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0917 10:19:12.735424   16648 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.2.0
RuntimeApiVersion:  v1
I0917 10:19:12.742047   16648 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0917 10:19:12.779654   16648 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0917 10:19:12.811327   16648 out.go:235] ðŸ³  Preparing Kubernetes v1.31.0 on Docker 27.2.0 ...
I0917 10:19:12.815603   16648 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0917 10:19:12.930507   16648 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0917 10:19:12.939614   16648 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0917 10:19:12.945538   16648 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0917 10:19:12.963672   16648 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0917 10:19:13.015751   16648 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\DELL:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0917 10:19:13.016286   16648 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I0917 10:19:13.022049   16648 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0917 10:19:13.050251   16648 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0917 10:19:13.050251   16648 docker.go:615] Images already preloaded, skipping extraction
I0917 10:19:13.054479   16648 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0917 10:19:13.089342   16648 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0917 10:19:13.089342   16648 cache_images.go:84] Images are preloaded, skipping loading
I0917 10:19:13.089342   16648 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.31.0 docker true true} ...
I0917 10:19:13.089342   16648 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.31.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0917 10:19:13.095741   16648 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0917 10:19:13.165703   16648 cni.go:84] Creating CNI manager for ""
I0917 10:19:13.165703   16648 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0917 10:19:13.165703   16648 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0917 10:19:13.165703   16648 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.31.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0917 10:19:13.166212   16648 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.31.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0917 10:19:13.173232   16648 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.31.0
I0917 10:19:13.186790   16648 binaries.go:44] Found k8s binaries, skipping transfer
I0917 10:19:13.197035   16648 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0917 10:19:13.209757   16648 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0917 10:19:13.263616   16648 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0917 10:19:13.314917   16648 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2150 bytes)
I0917 10:19:13.352665   16648 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0917 10:19:13.358505   16648 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0917 10:19:13.379869   16648 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0917 10:19:13.527605   16648 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0917 10:19:13.545790   16648 certs.go:68] Setting up C:\Users\DELL\.minikube\profiles\minikube for IP: 192.168.49.2
I0917 10:19:13.545790   16648 certs.go:194] generating shared ca certs ...
I0917 10:19:13.545790   16648 certs.go:226] acquiring lock for ca certs: {Name:mk065245d010ad3574bf79bb752816116d12e040 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0917 10:19:13.557649   16648 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\DELL\.minikube\ca.key
I0917 10:19:13.580953   16648 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\DELL\.minikube\proxy-client-ca.key
I0917 10:19:13.580953   16648 certs.go:256] generating profile certs ...
I0917 10:19:13.581986   16648 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": C:\Users\DELL\.minikube\profiles\minikube\client.key
I0917 10:19:13.610450   16648 certs.go:359] skipping valid signed profile cert regeneration for "minikube": C:\Users\DELL\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I0917 10:19:13.632265   16648 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": C:\Users\DELL\.minikube\profiles\minikube\proxy-client.key
I0917 10:19:13.636496   16648 certs.go:484] found cert: C:\Users\DELL\.minikube\certs\ca-key.pem (1679 bytes)
I0917 10:19:13.637034   16648 certs.go:484] found cert: C:\Users\DELL\.minikube\certs\ca.pem (1070 bytes)
I0917 10:19:13.637564   16648 certs.go:484] found cert: C:\Users\DELL\.minikube\certs\cert.pem (1115 bytes)
I0917 10:19:13.638113   16648 certs.go:484] found cert: C:\Users\DELL\.minikube\certs\key.pem (1679 bytes)
I0917 10:19:13.640329   16648 ssh_runner.go:362] scp C:\Users\DELL\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0917 10:19:13.669728   16648 ssh_runner.go:362] scp C:\Users\DELL\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0917 10:19:13.700111   16648 ssh_runner.go:362] scp C:\Users\DELL\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0917 10:19:13.730047   16648 ssh_runner.go:362] scp C:\Users\DELL\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0917 10:19:13.759295   16648 ssh_runner.go:362] scp C:\Users\DELL\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0917 10:19:13.797417   16648 ssh_runner.go:362] scp C:\Users\DELL\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0917 10:19:13.829009   16648 ssh_runner.go:362] scp C:\Users\DELL\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0917 10:19:13.857912   16648 ssh_runner.go:362] scp C:\Users\DELL\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0917 10:19:13.886256   16648 ssh_runner.go:362] scp C:\Users\DELL\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0917 10:19:13.915371   16648 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0917 10:19:13.942490   16648 ssh_runner.go:195] Run: openssl version
I0917 10:19:13.956158   16648 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0917 10:19:13.976812   16648 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0917 10:19:13.982676   16648 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Sep 15 11:20 /usr/share/ca-certificates/minikubeCA.pem
I0917 10:19:13.989603   16648 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0917 10:19:14.009555   16648 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0917 10:19:14.031641   16648 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0917 10:19:14.037682   16648 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0917 10:19:14.038744   16648 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\DELL:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0917 10:19:14.043045   16648 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0917 10:19:14.073584   16648 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0917 10:19:14.091219   16648 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0917 10:19:14.101990   16648 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I0917 10:19:14.111729   16648 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0917 10:19:14.123372   16648 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0917 10:19:14.123372   16648 kubeadm.go:157] found existing configuration files:

I0917 10:19:14.133547   16648 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0917 10:19:14.144903   16648 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0917 10:19:14.151704   16648 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0917 10:19:14.168176   16648 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0917 10:19:14.179213   16648 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0917 10:19:14.186218   16648 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0917 10:19:14.202950   16648 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0917 10:19:14.215661   16648 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0917 10:19:14.222159   16648 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0917 10:19:14.241896   16648 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0917 10:19:14.252592   16648 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0917 10:19:14.259148   16648 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0917 10:19:14.270289   16648 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.31.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0917 10:19:14.531343   16648 kubeadm.go:310] W0917 04:49:14.530668    2039 common.go:101] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta3" (kind: "ClusterConfiguration"). Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
I0917 10:19:14.532391   16648 kubeadm.go:310] W0917 04:49:14.532110    2039 common.go:101] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta3" (kind: "InitConfiguration"). Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
I0917 10:19:14.575005   16648 kubeadm.go:310] 	[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node
I0917 10:19:14.657238   16648 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0917 10:19:33.473020   16648 kubeadm.go:310] [init] Using Kubernetes version: v1.31.0
I0917 10:19:33.473020   16648 kubeadm.go:310] [preflight] Running pre-flight checks
I0917 10:19:33.473020   16648 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I0917 10:19:33.473551   16648 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0917 10:19:33.473551   16648 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I0917 10:19:33.474097   16648 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0917 10:19:33.476295   16648 out.go:235]     â–ª Generating certificates and keys ...
I0917 10:19:33.476831   16648 kubeadm.go:310] [certs] Using existing ca certificate authority
I0917 10:19:33.476831   16648 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I0917 10:19:33.476831   16648 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I0917 10:19:33.476831   16648 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I0917 10:19:33.477354   16648 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I0917 10:19:33.477354   16648 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I0917 10:19:33.477354   16648 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I0917 10:19:33.477354   16648 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0917 10:19:33.477883   16648 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I0917 10:19:33.477883   16648 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0917 10:19:33.478402   16648 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I0917 10:19:33.478402   16648 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I0917 10:19:33.478402   16648 kubeadm.go:310] [certs] Generating "sa" key and public key
I0917 10:19:33.478402   16648 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0917 10:19:33.479024   16648 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I0917 10:19:33.479024   16648 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0917 10:19:33.479024   16648 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0917 10:19:33.479540   16648 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0917 10:19:33.479540   16648 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0917 10:19:33.479540   16648 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0917 10:19:33.480073   16648 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0917 10:19:33.481147   16648 out.go:235]     â–ª Booting up control plane ...
I0917 10:19:33.481677   16648 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0917 10:19:33.481677   16648 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0917 10:19:33.482225   16648 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0917 10:19:33.482225   16648 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0917 10:19:33.482776   16648 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0917 10:19:33.482776   16648 kubeadm.go:310] [kubelet-start] Starting the kubelet
I0917 10:19:33.482776   16648 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0917 10:19:33.483321   16648 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I0917 10:19:33.483321   16648 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 1.503945794s
I0917 10:19:33.483321   16648 kubeadm.go:310] [api-check] Waiting for a healthy API server. This can take up to 4m0s
I0917 10:19:33.483850   16648 kubeadm.go:310] [api-check] The API server is healthy after 10.009793207s
I0917 10:19:33.483850   16648 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0917 10:19:33.483850   16648 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0917 10:19:33.484509   16648 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I0917 10:19:33.484509   16648 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0917 10:19:33.485017   16648 kubeadm.go:310] [bootstrap-token] Using token: sbsm64.0m3kt08feqmq7tji
I0917 10:19:33.486302   16648 out.go:235]     â–ª Configuring RBAC rules ...
I0917 10:19:33.486302   16648 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0917 10:19:33.486855   16648 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0917 10:19:33.487380   16648 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0917 10:19:33.487380   16648 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0917 10:19:33.487915   16648 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0917 10:19:33.487915   16648 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0917 10:19:33.487915   16648 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0917 10:19:33.488466   16648 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I0917 10:19:33.488466   16648 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I0917 10:19:33.488466   16648 kubeadm.go:310] 
I0917 10:19:33.488466   16648 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I0917 10:19:33.488466   16648 kubeadm.go:310] 
I0917 10:19:33.488999   16648 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I0917 10:19:33.488999   16648 kubeadm.go:310] 
I0917 10:19:33.488999   16648 kubeadm.go:310]   mkdir -p $HOME/.kube
I0917 10:19:33.488999   16648 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0917 10:19:33.488999   16648 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0917 10:19:33.488999   16648 kubeadm.go:310] 
I0917 10:19:33.489542   16648 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I0917 10:19:33.489542   16648 kubeadm.go:310] 
I0917 10:19:33.489542   16648 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0917 10:19:33.489542   16648 kubeadm.go:310] 
I0917 10:19:33.489542   16648 kubeadm.go:310] You should now deploy a pod network to the cluster.
I0917 10:19:33.489542   16648 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0917 10:19:33.490079   16648 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0917 10:19:33.490079   16648 kubeadm.go:310] 
I0917 10:19:33.490079   16648 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I0917 10:19:33.490625   16648 kubeadm.go:310] and service account keys on each node and then running the following as root:
I0917 10:19:33.490625   16648 kubeadm.go:310] 
I0917 10:19:33.490625   16648 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token sbsm64.0m3kt08feqmq7tji \
I0917 10:19:33.490625   16648 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:f9609768f0bd0a97eb8d4dc247a847b3a3874b9b64378a92e041f346e9ec614c \
I0917 10:19:33.490625   16648 kubeadm.go:310] 	--control-plane 
I0917 10:19:33.490625   16648 kubeadm.go:310] 
I0917 10:19:33.491155   16648 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I0917 10:19:33.491155   16648 kubeadm.go:310] 
I0917 10:19:33.491155   16648 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token sbsm64.0m3kt08feqmq7tji \
I0917 10:19:33.491684   16648 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:f9609768f0bd0a97eb8d4dc247a847b3a3874b9b64378a92e041f346e9ec614c 
I0917 10:19:33.491684   16648 cni.go:84] Creating CNI manager for ""
I0917 10:19:33.491684   16648 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0917 10:19:33.492813   16648 out.go:177] ðŸ”—  Configuring bridge CNI (Container Networking Interface) ...
I0917 10:19:33.505291   16648 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0917 10:19:33.548514   16648 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0917 10:19:33.584603   16648 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0917 10:19:33.597079   16648 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.31.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0917 10:19:33.600855   16648 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.31.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2024_09_17T10_19_33_0700 minikube.k8s.io/version=v1.34.0 minikube.k8s.io/commit=210b148df93a80eb872ecbeb7e35281b3c582c61 minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0917 10:19:33.629069   16648 ops.go:34] apiserver oom_adj: -16
I0917 10:19:34.129260   16648 kubeadm.go:1113] duration metric: took 544.1239ms to wait for elevateKubeSystemPrivileges
I0917 10:19:34.129260   16648 kubeadm.go:394] duration metric: took 20.0910522s to StartCluster
I0917 10:19:34.129772   16648 settings.go:142] acquiring lock: {Name:mkdce26dab5b02003dd3dd1b089da3b89ebae416 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0917 10:19:34.130320   16648 settings.go:150] Updating kubeconfig:  C:\Users\DELL\.kube\config
I0917 10:19:34.135742   16648 lock.go:35] WriteFile acquiring C:\Users\DELL\.kube\config: {Name:mkebc657628023f432578f979d2189a052d2709f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0917 10:19:34.137476   16648 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0917 10:19:34.137476   16648 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I0917 10:19:34.138055   16648 addons.go:507] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0917 10:19:34.138055   16648 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0917 10:19:34.138055   16648 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0917 10:19:34.138055   16648 addons.go:234] Setting addon storage-provisioner=true in "minikube"
W0917 10:19:34.138055   16648 addons.go:243] addon storage-provisioner should already be in state true
I0917 10:19:34.138055   16648 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0917 10:19:34.139098   16648 host.go:66] Checking if "minikube" exists ...
I0917 10:19:34.139098   16648 out.go:177] ðŸ”Ž  Verifying Kubernetes components...
I0917 10:19:34.164274   16648 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0917 10:19:34.165117   16648 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0917 10:19:34.170013   16648 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0917 10:19:34.243786   16648 out.go:177]     â–ª Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0917 10:19:34.245519   16648 addons.go:431] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0917 10:19:34.245519   16648 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0917 10:19:34.252957   16648 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 10:19:34.273093   16648 addons.go:234] Setting addon default-storageclass=true in "minikube"
W0917 10:19:34.273093   16648 addons.go:243] addon default-storageclass should already be in state true
I0917 10:19:34.273093   16648 host.go:66] Checking if "minikube" exists ...
I0917 10:19:34.287166   16648 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0917 10:19:34.332743   16648 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50875 SSHKeyPath:C:\Users\DELL\.minikube\machines\minikube\id_rsa Username:docker}
I0917 10:19:34.357497   16648 addons.go:431] installing /etc/kubernetes/addons/storageclass.yaml
I0917 10:19:34.357497   16648 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0917 10:19:34.364108   16648 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 10:19:34.445096   16648 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50875 SSHKeyPath:C:\Users\DELL\.minikube\machines\minikube\id_rsa Username:docker}
I0917 10:19:34.449615   16648 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0917 10:19:34.478103   16648 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0917 10:19:34.580907   16648 api_server.go:52] waiting for apiserver process to appear ...
I0917 10:19:34.588827   16648 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0917 10:19:34.644301   16648 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0917 10:19:34.653046   16648 api_server.go:72] duration metric: took 514.4589ms to wait for apiserver process to appear ...
I0917 10:19:34.653046   16648 api_server.go:88] waiting for apiserver healthz status ...
I0917 10:19:34.653259   16648 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50880/healthz ...
I0917 10:19:34.683657   16648 api_server.go:279] https://127.0.0.1:50880/healthz returned 200:
ok
I0917 10:19:34.726277   16648 api_server.go:141] control plane version: v1.31.0
I0917 10:19:34.726277   16648 api_server.go:131] duration metric: took 73.2312ms to wait for apiserver health ...
I0917 10:19:34.726277   16648 system_pods.go:43] waiting for kube-system pods to appear ...
I0917 10:19:34.750471   16648 system_pods.go:59] 4 kube-system pods found
I0917 10:19:34.751004   16648 system_pods.go:61] "etcd-minikube" [13e3cd63-c4da-4733-938c-7749c6ab7311] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0917 10:19:34.751004   16648 system_pods.go:61] "kube-apiserver-minikube" [1bc5edd7-e374-4f53-9da1-f2cffcce343a] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0917 10:19:34.751004   16648 system_pods.go:61] "kube-controller-manager-minikube" [5fa29284-3db7-4183-a3c5-9dedb0f37e39] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0917 10:19:34.751004   16648 system_pods.go:61] "kube-scheduler-minikube" [49161350-69c5-4976-be66-531b619159ce] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0917 10:19:34.751004   16648 system_pods.go:74] duration metric: took 24.7273ms to wait for pod list to return data ...
I0917 10:19:34.751004   16648 kubeadm.go:582] duration metric: took 612.9496ms to wait for: map[apiserver:true system_pods:true]
I0917 10:19:34.751004   16648 node_conditions.go:102] verifying NodePressure condition ...
I0917 10:19:34.757817   16648 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0917 10:19:34.757817   16648 node_conditions.go:123] node cpu capacity is 8
I0917 10:19:34.757817   16648 node_conditions.go:105] duration metric: took 6.8132ms to run NodePressure ...
I0917 10:19:34.757817   16648 start.go:241] waiting for startup goroutines ...
I0917 10:19:34.761647   16648 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0917 10:19:35.378405   16648 out.go:177] ðŸŒŸ  Enabled addons: storage-provisioner, default-storageclass
I0917 10:19:35.380071   16648 addons.go:510] duration metric: took 1.243232s for enable addons: enabled=[storage-provisioner default-storageclass]
I0917 10:19:35.380071   16648 start.go:246] waiting for cluster config update ...
I0917 10:19:35.380071   16648 start.go:255] writing updated cluster config ...
I0917 10:19:35.388770   16648 ssh_runner.go:195] Run: rm -f paused
I0917 10:19:35.574931   16648 start.go:600] kubectl: 1.30.2, cluster: 1.31.0 (minor skew: 1)
I0917 10:19:35.576506   16648 out.go:177] ðŸ„  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Sep 17 04:49:11 minikube dockerd[1457]: time="2024-09-17T04:49:11.331175796Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Sep 17 04:49:11 minikube dockerd[1457]: time="2024-09-17T04:49:11.405839658Z" level=info msg="Loading containers: start."
Sep 17 04:49:11 minikube dockerd[1457]: time="2024-09-17T04:49:11.590298549Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Sep 17 04:49:11 minikube dockerd[1457]: time="2024-09-17T04:49:11.654874804Z" level=info msg="Loading containers: done."
Sep 17 04:49:11 minikube dockerd[1457]: time="2024-09-17T04:49:11.699960999Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
Sep 17 04:49:11 minikube dockerd[1457]: time="2024-09-17T04:49:11.700026763Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
Sep 17 04:49:11 minikube dockerd[1457]: time="2024-09-17T04:49:11.700040511Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
Sep 17 04:49:11 minikube dockerd[1457]: time="2024-09-17T04:49:11.700048081Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Sep 17 04:49:11 minikube dockerd[1457]: time="2024-09-17T04:49:11.700080612Z" level=info msg="Docker daemon" commit=3ab5c7d containerd-snapshotter=false storage-driver=overlay2 version=27.2.0
Sep 17 04:49:11 minikube dockerd[1457]: time="2024-09-17T04:49:11.700132616Z" level=info msg="Daemon has completed initialization"
Sep 17 04:49:11 minikube dockerd[1457]: time="2024-09-17T04:49:11.757567833Z" level=info msg="API listen on /var/run/docker.sock"
Sep 17 04:49:11 minikube dockerd[1457]: time="2024-09-17T04:49:11.757737601Z" level=info msg="API listen on [::]:2376"
Sep 17 04:49:11 minikube systemd[1]: Started Docker Application Container Engine.
Sep 17 04:49:12 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Sep 17 04:49:12 minikube cri-dockerd[1727]: time="2024-09-17T04:49:12Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Sep 17 04:49:12 minikube cri-dockerd[1727]: time="2024-09-17T04:49:12Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Sep 17 04:49:12 minikube cri-dockerd[1727]: time="2024-09-17T04:49:12Z" level=info msg="Start docker client with request timeout 0s"
Sep 17 04:49:12 minikube cri-dockerd[1727]: time="2024-09-17T04:49:12Z" level=info msg="Hairpin mode is set to hairpin-veth"
Sep 17 04:49:12 minikube cri-dockerd[1727]: time="2024-09-17T04:49:12Z" level=info msg="Loaded network plugin cni"
Sep 17 04:49:12 minikube cri-dockerd[1727]: time="2024-09-17T04:49:12Z" level=info msg="Docker cri networking managed by network plugin cni"
Sep 17 04:49:12 minikube cri-dockerd[1727]: time="2024-09-17T04:49:12Z" level=info msg="Setting cgroupDriver cgroupfs"
Sep 17 04:49:12 minikube cri-dockerd[1727]: time="2024-09-17T04:49:12Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Sep 17 04:49:12 minikube cri-dockerd[1727]: time="2024-09-17T04:49:12Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Sep 17 04:49:12 minikube cri-dockerd[1727]: time="2024-09-17T04:49:12Z" level=info msg="Start cri-dockerd grpc backend"
Sep 17 04:49:12 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Sep 17 04:49:23 minikube cri-dockerd[1727]: time="2024-09-17T04:49:23Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d18ab8f5c14073f01f1222f0cc398180f1f55f63f911b3dbb02e4228af55aec9/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Sep 17 04:49:23 minikube cri-dockerd[1727]: time="2024-09-17T04:49:23Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/cd2e219238051525300b558eff92fa18bde7978ada474382e76f90529cc6a429/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Sep 17 04:49:23 minikube cri-dockerd[1727]: time="2024-09-17T04:49:23Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/036264e9064a93706a5c4e6842de1f7474fa7ee774dac33f0edbf9c7a89fd5cb/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Sep 17 04:49:23 minikube cri-dockerd[1727]: time="2024-09-17T04:49:23Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/fa7ec4cf27fb5905f3687a9df37f9c67c5c137df305d02e3543fc83719530306/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Sep 17 04:49:38 minikube cri-dockerd[1727]: time="2024-09-17T04:49:38Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8314dba4677bc847ce39b3f3c91ddf289c48e52653ce77b1ba2a4b5ef3d677f1/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Sep 17 04:49:38 minikube cri-dockerd[1727]: time="2024-09-17T04:49:38Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/33944900d6e823e523a2e8d7692d5b86db19fdaf987b8b4a746cfea04311f4c6/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Sep 17 04:49:39 minikube dockerd[1457]: time="2024-09-17T04:49:39.625820272Z" level=info msg="ignoring event" container=da41ce43abebcbde616e41117610ecfdba20742c1ee368fb016c99363c33a17e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 17 04:49:39 minikube cri-dockerd[1727]: time="2024-09-17T04:49:39Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b55a9d17b4a7a899a26c3cdbc1d8d568df024600ebed67bade8a7eabc9e3fb1a/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Sep 17 04:49:39 minikube cri-dockerd[1727]: time="2024-09-17T04:49:39Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2006312907c4f9287de26c1f34cd2998b747b6538bd645864c57cec5cb940fe5/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Sep 17 04:49:40 minikube dockerd[1457]: time="2024-09-17T04:49:40.429016252Z" level=info msg="ignoring event" container=fec7e1c1e5f59bd459cda8bed06f4d5786479e7160f8be82b194d30954dffd37 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 17 04:49:43 minikube cri-dockerd[1727]: time="2024-09-17T04:49:43Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Sep 17 04:54:31 minikube cri-dockerd[1727]: time="2024-09-17T04:54:31Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/eb0164b2ebc288e44e606bbef81c7a02dfff6a0725761e54354e145b374fccf3/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 17 04:54:32 minikube cri-dockerd[1727]: time="2024-09-17T04:54:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/813a626a6f1017bf6abcae75da38e909bc89f9fb898a52e75c233501e4fca244/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 17 04:54:32 minikube cri-dockerd[1727]: time="2024-09-17T04:54:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7910540e2c0b7b1172ad3a89be3ac928b56a1afc4ef3d3cfd56ca370141061f5/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 17 04:54:32 minikube cri-dockerd[1727]: time="2024-09-17T04:54:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9e351c29605634cfd7700b15cd2863d313d3fc180397fce26ae8baa7da20793e/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 17 04:54:32 minikube cri-dockerd[1727]: time="2024-09-17T04:54:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/46fe4bfe2eb6a38b1f32a79da0106794e303e1c2f040d58aab221e5ab68f3db0/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 17 04:54:32 minikube cri-dockerd[1727]: time="2024-09-17T04:54:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/47e9ba21cbcbd7ad9aea631282bc71afb1726e1a0be5899697cdede75c760bff/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 17 04:54:32 minikube cri-dockerd[1727]: time="2024-09-17T04:54:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/89f9497384b3736434ead7b263ddb26993327ab6eda9ee13a24c580172086e70/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 17 04:54:32 minikube cri-dockerd[1727]: time="2024-09-17T04:54:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/483b89db6fd947d8ecc13a8ad45140f2538a159767fd54e1b8b0a3ac2366e28e/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 17 04:54:32 minikube cri-dockerd[1727]: time="2024-09-17T04:54:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c9aa65513ac664e00b416cf524bec2f8039e289d20d50f09d081f2529b49c475/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 17 04:54:32 minikube cri-dockerd[1727]: time="2024-09-17T04:54:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/198532a61905dd06132d9adcf825cfd476d2653b3e1da3115893e4c639b49ed1/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 17 04:54:32 minikube cri-dockerd[1727]: time="2024-09-17T04:54:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4fb797d29bae90e0d3b7bda9ba3dcade630ae4a7f1762624878e1ae5a360406a/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 17 04:54:32 minikube cri-dockerd[1727]: time="2024-09-17T04:54:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3b8e838ace9be12e50fa8e78016a1c51853e1bca2cdc77626439a728bc6125e6/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 17 04:54:33 minikube cri-dockerd[1727]: time="2024-09-17T04:54:33Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1cde722f283da41db2c5d38ebcc89b1e271f3b8ae0f8822932d500a04bdc1db9/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 17 04:54:33 minikube cri-dockerd[1727]: time="2024-09-17T04:54:33Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6c563ba953050b796bd8be7bb0eeedc91ac14e5a20ec2ff42455a04beefd3592/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 17 04:54:33 minikube cri-dockerd[1727]: time="2024-09-17T04:54:33Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e685e468bc67eb6a200a654bcc984a01c7d397063f94b14d9d58e54e2e8fd5fb/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 17 04:54:33 minikube cri-dockerd[1727]: time="2024-09-17T04:54:33Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/bee2322e8aaf7a4c441f7a1671680a723bf393db1c915ddd03147a650b62410f/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 17 04:54:33 minikube cri-dockerd[1727]: time="2024-09-17T04:54:33Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/47674e10f8afd43f7f23baf58e75e3e2930d6bcba9bc21d22950cd31de0f2cff/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 17 04:54:33 minikube cri-dockerd[1727]: time="2024-09-17T04:54:33Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1c6c725823e83f01ba30477a9a6e6cfad3d5e7da81fe938f40e210807074d0bc/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 17 04:54:33 minikube cri-dockerd[1727]: time="2024-09-17T04:54:33Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/db0f3dd1c8850d4471f581fda7ad8a46f77bc854760eb4960ada9deb6ecb152f/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 17 04:54:33 minikube cri-dockerd[1727]: time="2024-09-17T04:54:33Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3dfa3a0c672484167ff02aa75d96d0843458ee70269e300a4223bb7f3e64cea5/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 17 04:54:50 minikube cri-dockerd[1727]: time="2024-09-17T04:54:50Z" level=info msg="Pulling image karthikshetty555/flask-kubernetes:latest: b47a222d28fa: Downloading [==>                                                ]  1.239MB/24.03MB"
Sep 17 04:55:00 minikube cri-dockerd[1727]: time="2024-09-17T04:55:00Z" level=info msg="Pulling image karthikshetty555/flask-kubernetes:latest: debce5f9f3a9: Downloading [==>                                                ]  3.226MB/64.11MB"
Sep 17 04:55:10 minikube cri-dockerd[1727]: time="2024-09-17T04:55:10Z" level=info msg="Pulling image karthikshetty555/flask-kubernetes:latest: b47a222d28fa: Downloading [===========>                                       ]  5.409MB/24.03MB"
Sep 17 04:55:20 minikube cri-dockerd[1727]: time="2024-09-17T04:55:20Z" level=info msg="Pulling image karthikshetty555/flask-kubernetes:latest: debce5f9f3a9: Downloading [======>                                            ]  8.071MB/64.11MB"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
e3626bb0547e8       6e38f40d628db       5 minutes ago       Running             storage-provisioner       2                   8314dba4677bc       storage-provisioner
55cf78bd5ba38       cbb01a7bd410d       5 minutes ago       Running             coredns                   0                   2006312907c4f       coredns-6f6b679f8f-fnsq6
9db9500ca7ad4       cbb01a7bd410d       5 minutes ago       Running             coredns                   0                   b55a9d17b4a7a       coredns-6f6b679f8f-whnnh
fec7e1c1e5f59       6e38f40d628db       5 minutes ago       Exited              storage-provisioner       1                   8314dba4677bc       storage-provisioner
10d7643dc5b49       ad83b2ca7b09e       5 minutes ago       Running             kube-proxy                0                   33944900d6e82       kube-proxy-h8trz
530f9a87cd4ab       1766f54c897f0       6 minutes ago       Running             kube-scheduler            0                   fa7ec4cf27fb5       kube-scheduler-minikube
9e5aa629dfbf0       2e96e5913fc06       6 minutes ago       Running             etcd                      0                   036264e9064a9       etcd-minikube
fd1a227acc15b       045733566833c       6 minutes ago       Running             kube-controller-manager   0                   cd2e219238051       kube-controller-manager-minikube
1fe7bedd86e4a       604f5db92eaa8       6 minutes ago       Running             kube-apiserver            0                   d18ab8f5c1407       kube-apiserver-minikube


==> coredns [55cf78bd5ba3] <==
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = 591cf328cccc12bc490481273e738df59329c62c0b729d94e8b61db9961c2fa5f046dd37f1cf888b953814040d180f52594972691cd6ff41be96639138a43908
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2


==> coredns [9db9500ca7ad] <==
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = 591cf328cccc12bc490481273e738df59329c62c0b729d94e8b61db9961c2fa5f046dd37f1cf888b953814040d180f52594972691cd6ff41be96639138a43908
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=210b148df93a80eb872ecbeb7e35281b3c582c61
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_09_17T10_19_33_0700
                    minikube.k8s.io/version=v1.34.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 17 Sep 2024 04:49:29 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Tue, 17 Sep 2024 04:55:21 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Tue, 17 Sep 2024 04:54:49 +0000   Tue, 17 Sep 2024 04:49:25 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 17 Sep 2024 04:54:49 +0000   Tue, 17 Sep 2024 04:49:25 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 17 Sep 2024 04:54:49 +0000   Tue, 17 Sep 2024 04:49:25 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Tue, 17 Sep 2024 04:54:49 +0000   Tue, 17 Sep 2024 04:49:29 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3941156Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3941156Ki
  pods:               110
System Info:
  Machine ID:                 7a04f67b1654497f815a88be8616a9e5
  System UUID:                7a04f67b1654497f815a88be8616a9e5
  Boot ID:                    e9642c73-d279-44e8-a03c-768140f0cc86
  Kernel Version:             5.15.153.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.2.0
  Kubelet Version:            v1.31.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (28 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     flask-test-app-7b6cd966fb-445km     0 (0%)        0 (0%)      0 (0%)           0 (0%)         74s
  default                     flask-test-app-7b6cd966fb-55nbq     0 (0%)        0 (0%)      0 (0%)           0 (0%)         74s
  default                     flask-test-app-7b6cd966fb-6vsql     0 (0%)        0 (0%)      0 (0%)           0 (0%)         74s
  default                     flask-test-app-7b6cd966fb-87hv2     0 (0%)        0 (0%)      0 (0%)           0 (0%)         74s
  default                     flask-test-app-7b6cd966fb-cdvmr     0 (0%)        0 (0%)      0 (0%)           0 (0%)         74s
  default                     flask-test-app-7b6cd966fb-dhc8x     0 (0%)        0 (0%)      0 (0%)           0 (0%)         74s
  default                     flask-test-app-7b6cd966fb-dk6xk     0 (0%)        0 (0%)      0 (0%)           0 (0%)         74s
  default                     flask-test-app-7b6cd966fb-dv5z7     0 (0%)        0 (0%)      0 (0%)           0 (0%)         74s
  default                     flask-test-app-7b6cd966fb-fdtsq     0 (0%)        0 (0%)      0 (0%)           0 (0%)         74s
  default                     flask-test-app-7b6cd966fb-fh8rb     0 (0%)        0 (0%)      0 (0%)           0 (0%)         74s
  default                     flask-test-app-7b6cd966fb-fpwld     0 (0%)        0 (0%)      0 (0%)           0 (0%)         74s
  default                     flask-test-app-7b6cd966fb-g7h78     0 (0%)        0 (0%)      0 (0%)           0 (0%)         74s
  default                     flask-test-app-7b6cd966fb-lst8m     0 (0%)        0 (0%)      0 (0%)           0 (0%)         74s
  default                     flask-test-app-7b6cd966fb-mgghv     0 (0%)        0 (0%)      0 (0%)           0 (0%)         74s
  default                     flask-test-app-7b6cd966fb-nskhp     0 (0%)        0 (0%)      0 (0%)           0 (0%)         74s
  default                     flask-test-app-7b6cd966fb-pkl9v     0 (0%)        0 (0%)      0 (0%)           0 (0%)         74s
  default                     flask-test-app-7b6cd966fb-pxtxl     0 (0%)        0 (0%)      0 (0%)           0 (0%)         74s
  default                     flask-test-app-7b6cd966fb-rnld2     0 (0%)        0 (0%)      0 (0%)           0 (0%)         74s
  default                     flask-test-app-7b6cd966fb-wdxcs     0 (0%)        0 (0%)      0 (0%)           0 (0%)         74s
  default                     flask-test-app-7b6cd966fb-wpd8s     0 (0%)        0 (0%)      0 (0%)           0 (0%)         74s
  kube-system                 coredns-6f6b679f8f-fnsq6            100m (1%)     0 (0%)      70Mi (1%)        170Mi (4%)     5m50s
  kube-system                 coredns-6f6b679f8f-whnnh            100m (1%)     0 (0%)      70Mi (1%)        170Mi (4%)     5m50s
  kube-system                 etcd-minikube                       100m (1%)     0 (0%)      100Mi (2%)       0 (0%)         5m57s
  kube-system                 kube-apiserver-minikube             250m (3%)     0 (0%)      0 (0%)           0 (0%)         5m55s
  kube-system                 kube-controller-manager-minikube    200m (2%)     0 (0%)      0 (0%)           0 (0%)         5m55s
  kube-system                 kube-proxy-h8trz                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         5m50s
  kube-system                 kube-scheduler-minikube             100m (1%)     0 (0%)      0 (0%)           0 (0%)         5m55s
  kube-system                 storage-provisioner                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         5m53s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (10%)  0 (0%)
  memory             240Mi (6%)  340Mi (8%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                             Age                  From             Message
  ----     ------                             ----                 ----             -------
  Normal   Starting                           5m47s                kube-proxy       
  Normal   NodeHasSufficientMemory            6m6s (x7 over 6m7s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              6m6s (x7 over 6m7s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               6m6s (x7 over 6m7s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced            6m6s                 kubelet          Updated Node Allocatable limit across pods
  Normal   Starting                           5m55s                kubelet          Starting kubelet.
  Warning  CgroupV1                           5m55s                kubelet          Cgroup v1 support is in maintenance mode, please migrate to Cgroup v2.
  Normal   NodeAllocatableEnforced            5m55s                kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory            5m55s                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              5m55s                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               5m55s                kubelet          Node minikube status is now: NodeHasSufficientPID
  Warning  PossibleMemoryBackedVolumesOnDisk  5m55s                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   RegisteredNode                     5m51s                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Sep17 04:46] PCI: Fatal: No config space access function found
[  +0.020700] PCI: System does not support PCI
[  +0.269927] kvm: already loaded the other module
[  +3.614490] FS-Cache: Duplicate cookie detected
[  +0.000737] FS-Cache: O-cookie c=00000005 [p=00000002 fl=222 nc=0 na=1]
[  +0.000579] FS-Cache: O-cookie d=000000006ada7ac7{9P.session} n=00000000a8149927
[  +0.001161] FS-Cache: O-key=[10] '34323934393337363838'
[  +0.000590] FS-Cache: N-cookie c=00000006 [p=00000002 fl=2 nc=0 na=1]
[  +0.000732] FS-Cache: N-cookie d=000000006ada7ac7{9P.session} n=00000000caf8279a
[  +0.000830] FS-Cache: N-key=[10] '34323934393337363838'
[  +1.011621] WSL (1) ERROR: ConfigApplyWindowsLibPath:2537: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000007]  failed 2
[  +0.026884] WSL (1) WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[  +0.817959] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.045899] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.013262] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.002397] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.017947] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +2.188317] netlink: 'init': attribute type 4 has an invalid length.
[Sep17 04:49] tmpfs: Unknown parameter 'noswap'
[ +11.302102] tmpfs: Unknown parameter 'noswap'


==> etcd [9e5aa629dfbf] <==
{"level":"warn","ts":"2024-09-17T04:49:24.930209Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-09-17T04:49:24.934596Z","caller":"mvcc/kvstore.go:418","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2024-09-17T04:49:24.938673Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-09-17T04:49:24.943285Z","caller":"etcdserver/server.go:867","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.15","cluster-version":"to_be_decided"}
{"level":"info","ts":"2024-09-17T04:49:24.944153Z","caller":"etcdserver/server.go:751","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2024-09-17T04:49:24.944958Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-09-17T04:49:24.945182Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-09-17T04:49:24.945199Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-09-17T04:49:24.944304Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-09-17T04:49:24.948321Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2024-09-17T04:49:24.948512Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-09-17T04:49:24.950435Z","caller":"embed/etcd.go:728","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-09-17T04:49:24.950881Z","caller":"embed/etcd.go:279","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-09-17T04:49:24.950941Z","caller":"embed/etcd.go:870","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-09-17T04:49:24.950651Z","caller":"embed/etcd.go:599","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-09-17T04:49:24.951806Z","caller":"embed/etcd.go:571","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-09-17T04:49:25.454477Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 1"}
{"level":"info","ts":"2024-09-17T04:49:25.455465Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 1"}
{"level":"info","ts":"2024-09-17T04:49:25.455652Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 1"}
{"level":"info","ts":"2024-09-17T04:49:25.455777Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 2"}
{"level":"info","ts":"2024-09-17T04:49:25.455878Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2024-09-17T04:49:25.455986Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 2"}
{"level":"info","ts":"2024-09-17T04:49:25.456073Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 2"}
{"level":"info","ts":"2024-09-17T04:49:25.469218Z","caller":"etcdserver/server.go:2629","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2024-09-17T04:49:25.475247Z","caller":"etcdserver/server.go:2118","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-09-17T04:49:25.475694Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-09-17T04:49:25.475311Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-09-17T04:49:25.478262Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-09-17T04:49:25.479374Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-09-17T04:49:25.480486Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-09-17T04:49:25.492567Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-09-17T04:49:25.493038Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2024-09-17T04:49:25.493422Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-09-17T04:49:25.493633Z","caller":"etcdserver/server.go:2653","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2024-09-17T04:49:25.482228Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-09-17T04:49:25.495118Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-09-17T04:49:37.528440Z","caller":"traceutil/trace.go:171","msg":"trace[1593508590] transaction","detail":"{read_only:false; response_revision:338; number_of_response:1; }","duration":"181.332367ms","start":"2024-09-17T04:49:37.347063Z","end":"2024-09-17T04:49:37.528395Z","steps":["trace[1593508590] 'process raft request'  (duration: 181.097463ms)"],"step_count":1}
{"level":"info","ts":"2024-09-17T04:49:37.528395Z","caller":"traceutil/trace.go:171","msg":"trace[1131871788] transaction","detail":"{read_only:false; response_revision:337; number_of_response:1; }","duration":"181.637612ms","start":"2024-09-17T04:49:37.346705Z","end":"2024-09-17T04:49:37.528342Z","steps":["trace[1131871788] 'process raft request'  (duration: 99.902966ms)","trace[1131871788] 'compare'  (duration: 81.391934ms)"],"step_count":2}
{"level":"info","ts":"2024-09-17T04:49:37.528598Z","caller":"traceutil/trace.go:171","msg":"trace[108761693] linearizableReadLoop","detail":"{readStateIndex:346; appliedIndex:344; }","duration":"104.905164ms","start":"2024-09-17T04:49:37.423663Z","end":"2024-09-17T04:49:37.528568Z","steps":["trace[108761693] 'read index received'  (duration: 22.567872ms)","trace[108761693] 'applied index is now lower than readState.Index'  (duration: 82.336002ms)"],"step_count":2}
{"level":"warn","ts":"2024-09-17T04:49:37.528927Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"105.081882ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/legacy-service-account-token-cleaner\" ","response":"range_response_count:1 size:238"}
{"level":"info","ts":"2024-09-17T04:49:37.529279Z","caller":"traceutil/trace.go:171","msg":"trace[616490576] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/legacy-service-account-token-cleaner; range_end:; response_count:1; response_revision:339; }","duration":"105.59122ms","start":"2024-09-17T04:49:37.423654Z","end":"2024-09-17T04:49:37.529245Z","steps":["trace[616490576] 'agreement among raft nodes before linearized reading'  (duration: 104.994783ms)"],"step_count":1}
{"level":"warn","ts":"2024-09-17T04:49:37.828197Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"104.296437ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128031948952795166 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/clusterroles/admin\" mod_revision:336 > success:<request_put:<key:\"/registry/clusterroles/admin\" value_size:2228 >> failure:<request_range:<key:\"/registry/clusterroles/admin\" > >>","response":"size:16"}
{"level":"info","ts":"2024-09-17T04:49:37.828494Z","caller":"traceutil/trace.go:171","msg":"trace[1891110467] transaction","detail":"{read_only:false; response_revision:341; number_of_response:1; }","duration":"205.162517ms","start":"2024-09-17T04:49:37.623304Z","end":"2024-09-17T04:49:37.828466Z","steps":["trace[1891110467] 'process raft request'  (duration: 99.908743ms)","trace[1891110467] 'compare'  (duration: 104.183173ms)"],"step_count":2}
{"level":"info","ts":"2024-09-17T04:49:37.828692Z","caller":"traceutil/trace.go:171","msg":"trace[575134022] transaction","detail":"{read_only:false; response_revision:342; number_of_response:1; }","duration":"180.462078ms","start":"2024-09-17T04:49:37.648173Z","end":"2024-09-17T04:49:37.828635Z","steps":["trace[575134022] 'process raft request'  (duration: 180.326105ms)"],"step_count":1}
{"level":"info","ts":"2024-09-17T04:49:37.828721Z","caller":"traceutil/trace.go:171","msg":"trace[180666908] linearizableReadLoop","detail":"{readStateIndex:350; appliedIndex:349; }","duration":"194.614099ms","start":"2024-09-17T04:49:37.634083Z","end":"2024-09-17T04:49:37.828698Z","steps":["trace[180666908] 'read index received'  (duration: 89.184665ms)","trace[180666908] 'applied index is now lower than readState.Index'  (duration: 105.4255ms)"],"step_count":2}
{"level":"warn","ts":"2024-09-17T04:49:37.828919Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"184.166429ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/endpoint-controller\" ","response":"range_response_count:1 size:203"}
{"level":"info","ts":"2024-09-17T04:49:37.828984Z","caller":"traceutil/trace.go:171","msg":"trace[1089939283] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/endpoint-controller; range_end:; response_count:1; response_revision:342; }","duration":"184.23993ms","start":"2024-09-17T04:49:37.644731Z","end":"2024-09-17T04:49:37.828971Z","steps":["trace[1089939283] 'agreement among raft nodes before linearized reading'  (duration: 184.140812ms)"],"step_count":1}
{"level":"warn","ts":"2024-09-17T04:49:37.828925Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"194.929277ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/service-account-controller\" ","response":"range_response_count:1 size:218"}
{"level":"info","ts":"2024-09-17T04:49:37.829449Z","caller":"traceutil/trace.go:171","msg":"trace[1470650600] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/service-account-controller; range_end:; response_count:1; response_revision:342; }","duration":"195.463337ms","start":"2024-09-17T04:49:37.633968Z","end":"2024-09-17T04:49:37.829432Z","steps":["trace[1470650600] 'agreement among raft nodes before linearized reading'  (duration: 194.88512ms)"],"step_count":1}
{"level":"warn","ts":"2024-09-17T04:53:33.718976Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"128.040141ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128031948952796443 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:612 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:512 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:16"}
{"level":"info","ts":"2024-09-17T04:53:33.727716Z","caller":"traceutil/trace.go:171","msg":"trace[27454520] transaction","detail":"{read_only:false; response_revision:614; number_of_response:1; }","duration":"228.758679ms","start":"2024-09-17T04:53:33.497109Z","end":"2024-09-17T04:53:33.725868Z","steps":["trace[27454520] 'get key's previous created_revision and leaseID' {req_type:put; key:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; req_size:582; } (duration: 105.196502ms)"],"step_count":1}
{"level":"info","ts":"2024-09-17T04:54:00.047225Z","caller":"traceutil/trace.go:171","msg":"trace[1388686894] transaction","detail":"{read_only:false; response_revision:635; number_of_response:1; }","duration":"131.507053ms","start":"2024-09-17T04:53:59.914962Z","end":"2024-09-17T04:54:00.046469Z","steps":["trace[1388686894] 'process raft request'  (duration: 131.288917ms)"],"step_count":1}
{"level":"warn","ts":"2024-09-17T04:54:12.331408Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"190.955707ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:600"}
{"level":"info","ts":"2024-09-17T04:54:12.331610Z","caller":"traceutil/trace.go:171","msg":"trace[873987023] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:644; }","duration":"191.999702ms","start":"2024-09-17T04:54:12.139541Z","end":"2024-09-17T04:54:12.331541Z","steps":["trace[873987023] 'range keys from in-memory index tree'  (duration: 189.837189ms)"],"step_count":1}
{"level":"info","ts":"2024-09-17T04:54:14.819032Z","caller":"traceutil/trace.go:171","msg":"trace[1028633038] linearizableReadLoop","detail":"{readStateIndex:741; appliedIndex:740; }","duration":"102.887563ms","start":"2024-09-17T04:54:14.716046Z","end":"2024-09-17T04:54:14.818933Z","steps":["trace[1028633038] 'read index received'  (duration: 9.406518ms)","trace[1028633038] 'applied index is now lower than readState.Index'  (duration: 93.479444ms)"],"step_count":2}
{"level":"info","ts":"2024-09-17T04:54:14.819106Z","caller":"traceutil/trace.go:171","msg":"trace[1283470273] transaction","detail":"{read_only:false; response_revision:675; number_of_response:1; }","duration":"105.63839ms","start":"2024-09-17T04:54:14.713312Z","end":"2024-09-17T04:54:14.818950Z","steps":["trace[1283470273] 'process raft request'  (duration: 92.601253ms)","trace[1283470273] 'compare'  (duration: 12.319915ms)"],"step_count":2}
{"level":"warn","ts":"2024-09-17T04:54:14.819876Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"103.818726ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-09-17T04:54:14.820964Z","caller":"traceutil/trace.go:171","msg":"trace[1733335463] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:676; }","duration":"103.90393ms","start":"2024-09-17T04:54:14.716019Z","end":"2024-09-17T04:54:14.819923Z","steps":["trace[1733335463] 'agreement among raft nodes before linearized reading'  (duration: 103.733449ms)"],"step_count":1}
{"level":"info","ts":"2024-09-17T04:54:15.213205Z","caller":"traceutil/trace.go:171","msg":"trace[1603267090] transaction","detail":"{read_only:false; response_revision:707; number_of_response:1; }","duration":"107.469697ms","start":"2024-09-17T04:54:15.105697Z","end":"2024-09-17T04:54:15.213166Z","steps":["trace[1603267090] 'process raft request'  (duration: 26.503007ms)","trace[1603267090] 'compare'  (duration: 74.532966ms)"],"step_count":2}
{"level":"info","ts":"2024-09-17T04:54:21.914392Z","caller":"traceutil/trace.go:171","msg":"trace[599381642] transaction","detail":"{read_only:false; response_revision:765; number_of_response:1; }","duration":"100.641947ms","start":"2024-09-17T04:54:21.813708Z","end":"2024-09-17T04:54:21.914350Z","steps":["trace[599381642] 'compare'  (duration: 95.722119ms)"],"step_count":1}


==> kernel <==
 04:55:28 up 8 min,  0 users,  load average: 2.30, 1.51, 0.76
Linux minikube 5.15.153.1-microsoft-standard-WSL2 #1 SMP Fri Mar 29 23:14:13 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [1fe7bedd86e4] <==
I0917 04:49:29.258141       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0917 04:49:29.258401       1 cluster_authentication_trust_controller.go:443] Starting cluster_authentication_trust_controller controller
I0917 04:49:29.258450       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0917 04:49:29.258485       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I0917 04:49:29.258501       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0917 04:49:29.258533       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I0917 04:49:29.258646       1 controller.go:119] Starting legacy_token_tracking_controller
I0917 04:49:29.258685       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0917 04:49:29.258709       1 aggregator.go:169] waiting for initial CRD sync...
I0917 04:49:29.261198       1 system_namespaces_controller.go:66] Starting system namespaces controller
I0917 04:49:29.261430       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I0917 04:49:29.261737       1 local_available_controller.go:156] Starting LocalAvailability controller
I0917 04:49:29.261771       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I0917 04:49:29.265743       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0917 04:49:29.265957       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0917 04:49:29.266428       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0917 04:49:29.266486       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0917 04:49:29.323701       1 controller.go:142] Starting OpenAPI controller
I0917 04:49:29.323863       1 controller.go:90] Starting OpenAPI V3 controller
I0917 04:49:29.323917       1 naming_controller.go:294] Starting NamingConditionController
I0917 04:49:29.323957       1 establishing_controller.go:81] Starting EstablishingController
I0917 04:49:29.324000       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0917 04:49:29.324035       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0917 04:49:29.324073       1 crd_finalizer.go:269] Starting CRDFinalizer
I0917 04:49:29.349790       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0917 04:49:29.349841       1 policy_source.go:224] refreshing policies
I0917 04:49:29.358563       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0917 04:49:29.358796       1 shared_informer.go:320] Caches are synced for configmaps
I0917 04:49:29.359077       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0917 04:49:29.359114       1 handler_discovery.go:450] Starting ResourceDiscoveryManager
I0917 04:49:29.359148       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0917 04:49:29.359156       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0917 04:49:29.360775       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0917 04:49:29.423000       1 shared_informer.go:320] Caches are synced for node_authorizer
I0917 04:49:29.423268       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0917 04:49:29.423537       1 aggregator.go:171] initial CRD sync complete...
I0917 04:49:29.423549       1 autoregister_controller.go:144] Starting autoregister controller
I0917 04:49:29.423560       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0917 04:49:29.423569       1 cache.go:39] Caches are synced for autoregister controller
I0917 04:49:29.424613       1 cache.go:39] Caches are synced for LocalAvailability controller
I0917 04:49:29.426622       1 controller.go:615] quota admission added evaluator for: namespaces
E0917 04:49:29.551521       1 controller.go:148] "Unhandled Error" err="while syncing ConfigMap \"kube-system/kube-apiserver-legacy-service-account-token-tracking\", err: namespaces \"kube-system\" not found" logger="UnhandledError"
E0917 04:49:29.551538       1 controller.go:145] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
I0917 04:49:29.759234       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0917 04:49:30.265676       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0917 04:49:30.275193       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0917 04:49:30.275246       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0917 04:49:31.482600       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0917 04:49:31.572357       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0917 04:49:31.748613       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0917 04:49:31.762338       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0917 04:49:31.764199       1 controller.go:615] quota admission added evaluator for: endpoints
I0917 04:49:31.772539       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0917 04:49:32.468652       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0917 04:49:32.977925       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0917 04:49:32.999488       1 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0917 04:49:33.037038       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I0917 04:49:37.962628       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0917 04:49:38.163666       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
I0917 04:54:14.329500       1 alloc.go:330] "allocated clusterIPs" service="default/flask-test-service" clusterIPs={"IPv4":"10.109.95.25"}


==> kube-controller-manager [fd1a227acc15] <==
I0917 04:49:37.385134       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0917 04:49:37.423176       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0917 04:49:37.424506       1 shared_informer.go:320] Caches are synced for namespace
I0917 04:49:37.424905       1 shared_informer.go:320] Caches are synced for taint
I0917 04:49:37.425282       1 shared_informer.go:320] Caches are synced for persistent volume
I0917 04:49:37.433102       1 node_lifecycle_controller.go:1232] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0917 04:49:37.435701       1 node_lifecycle_controller.go:884] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0917 04:49:37.442437       1 shared_informer.go:320] Caches are synced for daemon sets
I0917 04:49:37.443053       1 shared_informer.go:320] Caches are synced for node
I0917 04:49:37.443609       1 range_allocator.go:171] "Sending events to api server" logger="node-ipam-controller"
I0917 04:49:37.444100       1 range_allocator.go:177] "Starting range CIDR allocator" logger="node-ipam-controller"
I0917 04:49:37.444182       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0917 04:49:37.444263       1 shared_informer.go:320] Caches are synced for cidrallocator
I0917 04:49:37.438372       1 node_lifecycle_controller.go:1078] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0917 04:49:37.457177       1 shared_informer.go:320] Caches are synced for disruption
I0917 04:49:37.457447       1 shared_informer.go:320] Caches are synced for resource quota
I0917 04:49:37.541163       1 shared_informer.go:320] Caches are synced for resource quota
I0917 04:49:37.546328       1 range_allocator.go:422] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I0917 04:49:37.546392       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0917 04:49:37.547490       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0917 04:49:37.842785       1 shared_informer.go:320] Caches are synced for garbage collector
I0917 04:49:37.846772       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0917 04:49:37.923789       1 shared_informer.go:320] Caches are synced for garbage collector
I0917 04:49:37.923865       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I0917 04:49:38.122071       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0917 04:49:38.356947       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="388.826233ms"
I0917 04:49:38.374965       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="17.773094ms"
I0917 04:49:38.375132       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="76.055Âµs"
I0917 04:49:38.379438       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="83.593Âµs"
I0917 04:49:40.931650       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="96.584Âµs"
I0917 04:49:40.986178       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="104.026Âµs"
I0917 04:49:42.269007       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="20.036941ms"
I0917 04:49:42.270616       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="113.428Âµs"
I0917 04:49:43.972541       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0917 04:49:48.373426       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="34.841601ms"
I0917 04:49:48.373683       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="105.566Âµs"
I0917 04:54:15.132910       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-test-app-7b6cd966fb" duration="617.151388ms"
I0917 04:54:15.229771       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-test-app-7b6cd966fb" duration="23.774831ms"
I0917 04:54:15.230000       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-test-app-7b6cd966fb" duration="145.007Âµs"
I0917 04:54:15.408467       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-test-app-7b6cd966fb" duration="167.883Âµs"
I0917 04:54:15.518588       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-test-app-7b6cd966fb" duration="285.9Âµs"
I0917 04:54:15.819689       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-test-app-7b6cd966fb" duration="830.648Âµs"
I0917 04:54:16.109897       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-test-app-7b6cd966fb" duration="151.641Âµs"
I0917 04:54:16.236961       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-test-app-7b6cd966fb" duration="226.225Âµs"
I0917 04:54:16.406482       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-test-app-7b6cd966fb" duration="169.409Âµs"
I0917 04:54:16.525922       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-test-app-7b6cd966fb" duration="338.016Âµs"
I0917 04:54:16.712587       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-test-app-7b6cd966fb" duration="201.04Âµs"
I0917 04:54:16.930895       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-test-app-7b6cd966fb" duration="169.692Âµs"
I0917 04:54:17.115298       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-test-app-7b6cd966fb" duration="1.408496ms"
I0917 04:54:17.216165       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-test-app-7b6cd966fb" duration="157.172Âµs"
I0917 04:54:17.242338       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-test-app-7b6cd966fb" duration="351.934Âµs"
I0917 04:54:17.309346       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-test-app-7b6cd966fb" duration="930.614Âµs"
I0917 04:54:17.436535       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-test-app-7b6cd966fb" duration="549.499Âµs"
I0917 04:54:17.518435       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-test-app-7b6cd966fb" duration="177.332Âµs"
I0917 04:54:17.551148       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-test-app-7b6cd966fb" duration="291.975Âµs"
I0917 04:54:17.637842       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-test-app-7b6cd966fb" duration="155.164Âµs"
I0917 04:54:17.714013       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-test-app-7b6cd966fb" duration="477.588Âµs"
I0917 04:54:17.820804       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-test-app-7b6cd966fb" duration="217.798Âµs"
I0917 04:54:17.927670       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flask-test-app-7b6cd966fb" duration="318.232Âµs"
I0917 04:54:49.749825       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-proxy [10d7643dc5b4] <==
E0917 04:49:40.325297       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0917 04:49:40.341923       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0917 04:49:40.435928       1 server_linux.go:66] "Using iptables proxy"
I0917 04:49:40.793280       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0917 04:49:40.793454       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0917 04:49:40.845412       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0917 04:49:40.845544       1 server_linux.go:169] "Using iptables Proxier"
I0917 04:49:40.850373       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0917 04:49:40.862233       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0917 04:49:40.874826       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0917 04:49:40.875160       1 server.go:483] "Version info" version="v1.31.0"
I0917 04:49:40.875231       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0917 04:49:40.879105       1 config.go:197] "Starting service config controller"
I0917 04:49:40.879270       1 config.go:104] "Starting endpoint slice config controller"
I0917 04:49:40.879341       1 config.go:326] "Starting node config controller"
I0917 04:49:40.880440       1 shared_informer.go:313] Waiting for caches to sync for service config
I0917 04:49:40.880685       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0917 04:49:40.880717       1 shared_informer.go:313] Waiting for caches to sync for node config
I0917 04:49:40.981612       1 shared_informer.go:320] Caches are synced for service config
I0917 04:49:40.981638       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0917 04:49:40.981671       1 shared_informer.go:320] Caches are synced for node config


==> kube-scheduler [530f9a87cd4a] <==
I0917 04:49:29.449908       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
W0917 04:49:29.451777       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0917 04:49:29.451848       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0917 04:49:29.523692       1 reflector.go:561] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0917 04:49:29.523791       1 reflector.go:158] "Unhandled Error" err="runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0917 04:49:29.523908       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0917 04:49:29.523911       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0917 04:49:29.523941       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0917 04:49:29.523965       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0917 04:49:29.523956       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0917 04:49:29.524025       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0917 04:49:29.523991       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
E0917 04:49:29.524048       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0917 04:49:29.524132       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0917 04:49:29.524134       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0917 04:49:29.524152       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
E0917 04:49:29.524156       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0917 04:49:29.524167       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0917 04:49:29.524307       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0917 04:49:29.524338       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0917 04:49:29.524359       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0917 04:49:29.524441       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0917 04:49:29.524463       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0917 04:49:29.524510       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0917 04:49:29.524515       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0917 04:49:29.524532       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
E0917 04:49:29.524536       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0917 04:49:29.524242       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0917 04:49:29.524641       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0917 04:49:29.524249       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0917 04:49:29.524678       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0917 04:49:30.335555       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0917 04:49:30.335641       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0917 04:49:30.377174       1 reflector.go:561] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0917 04:49:30.377256       1 reflector.go:158] "Unhandled Error" err="runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0917 04:49:30.529953       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0917 04:49:30.530033       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0917 04:49:30.549909       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0917 04:49:30.549986       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0917 04:49:30.585762       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0917 04:49:30.585903       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0917 04:49:30.645422       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0917 04:49:30.645506       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0917 04:49:30.645422       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0917 04:49:30.645610       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0917 04:49:30.831331       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0917 04:49:30.831414       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0917 04:49:30.870997       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0917 04:49:30.871084       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0917 04:49:30.897948       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0917 04:49:30.898032       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0917 04:49:30.943430       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0917 04:49:30.943493       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0917 04:49:31.022679       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0917 04:49:31.022787       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0917 04:49:31.062287       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0917 04:49:31.062359       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0917 04:49:31.078838       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0917 04:49:31.078929       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
I0917 04:49:32.750167       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Sep 17 04:49:34 minikube kubelet[2620]: I0917 04:49:34.429167    2620 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-scheduler-minikube" podStartSLOduration=1.429152691 podStartE2EDuration="1.429152691s" podCreationTimestamp="2024-09-17 04:49:33 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-09-17 04:49:34.393346365 +0000 UTC m=+1.456549026" watchObservedRunningTime="2024-09-17 04:49:34.429152691 +0000 UTC m=+1.492355352"
Sep 17 04:49:34 minikube kubelet[2620]: I0917 04:49:34.476593    2620 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-controller-manager-minikube" podStartSLOduration=1.476556586 podStartE2EDuration="1.476556586s" podCreationTimestamp="2024-09-17 04:49:33 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-09-17 04:49:34.476092817 +0000 UTC m=+1.539295482" watchObservedRunningTime="2024-09-17 04:49:34.476556586 +0000 UTC m=+1.539759254"
Sep 17 04:49:38 minikube kubelet[2620]: I0917 04:49:38.029368    2620 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/c735a312-0b8d-42b1-8498-c7abebe8ded5-tmp\") pod \"storage-provisioner\" (UID: \"c735a312-0b8d-42b1-8498-c7abebe8ded5\") " pod="kube-system/storage-provisioner"
Sep 17 04:49:38 minikube kubelet[2620]: I0917 04:49:38.029441    2620 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-2bdfd\" (UniqueName: \"kubernetes.io/projected/c735a312-0b8d-42b1-8498-c7abebe8ded5-kube-api-access-2bdfd\") pod \"storage-provisioner\" (UID: \"c735a312-0b8d-42b1-8498-c7abebe8ded5\") " pod="kube-system/storage-provisioner"
Sep 17 04:49:38 minikube kubelet[2620]: I0917 04:49:38.332084    2620 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/54a6fb93-1817-4622-aabf-dd8fe73ac21a-kube-proxy\") pod \"kube-proxy-h8trz\" (UID: \"54a6fb93-1817-4622-aabf-dd8fe73ac21a\") " pod="kube-system/kube-proxy-h8trz"
Sep 17 04:49:38 minikube kubelet[2620]: I0917 04:49:38.332175    2620 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/54a6fb93-1817-4622-aabf-dd8fe73ac21a-xtables-lock\") pod \"kube-proxy-h8trz\" (UID: \"54a6fb93-1817-4622-aabf-dd8fe73ac21a\") " pod="kube-system/kube-proxy-h8trz"
Sep 17 04:49:38 minikube kubelet[2620]: I0917 04:49:38.332205    2620 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/54a6fb93-1817-4622-aabf-dd8fe73ac21a-lib-modules\") pod \"kube-proxy-h8trz\" (UID: \"54a6fb93-1817-4622-aabf-dd8fe73ac21a\") " pod="kube-system/kube-proxy-h8trz"
Sep 17 04:49:38 minikube kubelet[2620]: I0917 04:49:38.332237    2620 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-xfflv\" (UniqueName: \"kubernetes.io/projected/54a6fb93-1817-4622-aabf-dd8fe73ac21a-kube-api-access-xfflv\") pod \"kube-proxy-h8trz\" (UID: \"54a6fb93-1817-4622-aabf-dd8fe73ac21a\") " pod="kube-system/kube-proxy-h8trz"
Sep 17 04:49:38 minikube kubelet[2620]: I0917 04:49:38.432886    2620 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-rbdlb\" (UniqueName: \"kubernetes.io/projected/36575df1-a6d8-43d0-ae76-5e1aa9f03ab2-kube-api-access-rbdlb\") pod \"coredns-6f6b679f8f-whnnh\" (UID: \"36575df1-a6d8-43d0-ae76-5e1aa9f03ab2\") " pod="kube-system/coredns-6f6b679f8f-whnnh"
Sep 17 04:49:38 minikube kubelet[2620]: I0917 04:49:38.433002    2620 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/834b35a9-48e6-4570-8eec-ed1ae0d8c9a1-config-volume\") pod \"coredns-6f6b679f8f-fnsq6\" (UID: \"834b35a9-48e6-4570-8eec-ed1ae0d8c9a1\") " pod="kube-system/coredns-6f6b679f8f-fnsq6"
Sep 17 04:49:38 minikube kubelet[2620]: I0917 04:49:38.433087    2620 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-2nhvn\" (UniqueName: \"kubernetes.io/projected/834b35a9-48e6-4570-8eec-ed1ae0d8c9a1-kube-api-access-2nhvn\") pod \"coredns-6f6b679f8f-fnsq6\" (UID: \"834b35a9-48e6-4570-8eec-ed1ae0d8c9a1\") " pod="kube-system/coredns-6f6b679f8f-fnsq6"
Sep 17 04:49:38 minikube kubelet[2620]: I0917 04:49:38.433119    2620 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/36575df1-a6d8-43d0-ae76-5e1aa9f03ab2-config-volume\") pod \"coredns-6f6b679f8f-whnnh\" (UID: \"36575df1-a6d8-43d0-ae76-5e1aa9f03ab2\") " pod="kube-system/coredns-6f6b679f8f-whnnh"
Sep 17 04:49:39 minikube kubelet[2620]: I0917 04:49:39.780105    2620 scope.go:117] "RemoveContainer" containerID="da41ce43abebcbde616e41117610ecfdba20742c1ee368fb016c99363c33a17e"
Sep 17 04:49:39 minikube kubelet[2620]: I0917 04:49:39.829129    2620 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="b55a9d17b4a7a899a26c3cdbc1d8d568df024600ebed67bade8a7eabc9e3fb1a"
Sep 17 04:49:39 minikube kubelet[2620]: I0917 04:49:39.840750    2620 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="2006312907c4f9287de26c1f34cd2998b747b6538bd645864c57cec5cb940fe5"
Sep 17 04:49:40 minikube kubelet[2620]: I0917 04:49:40.930337    2620 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-proxy-h8trz" podStartSLOduration=2.9302922479999998 podStartE2EDuration="2.930292248s" podCreationTimestamp="2024-09-17 04:49:38 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-09-17 04:49:39.964930302 +0000 UTC m=+7.028132978" watchObservedRunningTime="2024-09-17 04:49:40.930292248 +0000 UTC m=+7.993494925"
Sep 17 04:49:40 minikube kubelet[2620]: I0917 04:49:40.930702    2620 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/coredns-6f6b679f8f-fnsq6" podStartSLOduration=2.930673494 podStartE2EDuration="2.930673494s" podCreationTimestamp="2024-09-17 04:49:38 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-09-17 04:49:40.930249087 +0000 UTC m=+7.993451760" watchObservedRunningTime="2024-09-17 04:49:40.930673494 +0000 UTC m=+7.993876595"
Sep 17 04:49:40 minikube kubelet[2620]: I0917 04:49:40.935005    2620 scope.go:117] "RemoveContainer" containerID="da41ce43abebcbde616e41117610ecfdba20742c1ee368fb016c99363c33a17e"
Sep 17 04:49:40 minikube kubelet[2620]: I0917 04:49:40.935344    2620 scope.go:117] "RemoveContainer" containerID="fec7e1c1e5f59bd459cda8bed06f4d5786479e7160f8be82b194d30954dffd37"
Sep 17 04:49:40 minikube kubelet[2620]: E0917 04:49:40.935579    2620 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(c735a312-0b8d-42b1-8498-c7abebe8ded5)\"" pod="kube-system/storage-provisioner" podUID="c735a312-0b8d-42b1-8498-c7abebe8ded5"
Sep 17 04:49:40 minikube kubelet[2620]: I0917 04:49:40.965214    2620 scope.go:117] "RemoveContainer" containerID="da41ce43abebcbde616e41117610ecfdba20742c1ee368fb016c99363c33a17e"
Sep 17 04:49:40 minikube kubelet[2620]: E0917 04:49:40.967181    2620 log.go:32] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: da41ce43abebcbde616e41117610ecfdba20742c1ee368fb016c99363c33a17e" containerID="da41ce43abebcbde616e41117610ecfdba20742c1ee368fb016c99363c33a17e"
Sep 17 04:49:40 minikube kubelet[2620]: I0917 04:49:40.967281    2620 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"da41ce43abebcbde616e41117610ecfdba20742c1ee368fb016c99363c33a17e"} err="failed to get container status \"da41ce43abebcbde616e41117610ecfdba20742c1ee368fb016c99363c33a17e\": rpc error: code = Unknown desc = Error response from daemon: No such container: da41ce43abebcbde616e41117610ecfdba20742c1ee368fb016c99363c33a17e"
Sep 17 04:49:41 minikube kubelet[2620]: I0917 04:49:41.625414    2620 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/coredns-6f6b679f8f-whnnh" podStartSLOduration=3.6253766990000003 podStartE2EDuration="3.625376699s" podCreationTimestamp="2024-09-17 04:49:38 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-09-17 04:49:40.985818949 +0000 UTC m=+8.049021626" watchObservedRunningTime="2024-09-17 04:49:41.625376699 +0000 UTC m=+8.688579378"
Sep 17 04:49:41 minikube kubelet[2620]: I0917 04:49:41.976364    2620 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Sep 17 04:49:41 minikube kubelet[2620]: I0917 04:49:41.976562    2620 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Sep 17 04:49:41 minikube kubelet[2620]: I0917 04:49:41.977874    2620 scope.go:117] "RemoveContainer" containerID="fec7e1c1e5f59bd459cda8bed06f4d5786479e7160f8be82b194d30954dffd37"
Sep 17 04:49:41 minikube kubelet[2620]: E0917 04:49:41.978150    2620 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(c735a312-0b8d-42b1-8498-c7abebe8ded5)\"" pod="kube-system/storage-provisioner" podUID="c735a312-0b8d-42b1-8498-c7abebe8ded5"
Sep 17 04:49:43 minikube kubelet[2620]: I0917 04:49:43.956602    2620 kuberuntime_manager.go:1633] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Sep 17 04:49:43 minikube kubelet[2620]: I0917 04:49:43.957991    2620 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Sep 17 04:49:48 minikube kubelet[2620]: I0917 04:49:48.306579    2620 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Sep 17 04:49:57 minikube kubelet[2620]: I0917 04:49:57.260937    2620 scope.go:117] "RemoveContainer" containerID="fec7e1c1e5f59bd459cda8bed06f4d5786479e7160f8be82b194d30954dffd37"
Sep 17 04:49:58 minikube kubelet[2620]: I0917 04:49:58.208910    2620 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/storage-provisioner" podStartSLOduration=23.208188833 podStartE2EDuration="23.208188833s" podCreationTimestamp="2024-09-17 04:49:35 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-09-17 04:49:58.207773748 +0000 UTC m=+25.273297724" watchObservedRunningTime="2024-09-17 04:49:58.208188833 +0000 UTC m=+25.273712814"
Sep 17 04:54:15 minikube kubelet[2620]: I0917 04:54:15.125389    2620 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-s5pss\" (UniqueName: \"kubernetes.io/projected/40e530f7-d004-454c-8369-4953c6c04ff9-kube-api-access-s5pss\") pod \"flask-test-app-7b6cd966fb-rnld2\" (UID: \"40e530f7-d004-454c-8369-4953c6c04ff9\") " pod="default/flask-test-app-7b6cd966fb-rnld2"
Sep 17 04:54:15 minikube kubelet[2620]: I0917 04:54:15.132134    2620 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-nd4cz\" (UniqueName: \"kubernetes.io/projected/cd3d5358-af96-4d35-9091-75d0cb457bf2-kube-api-access-nd4cz\") pod \"flask-test-app-7b6cd966fb-pxtxl\" (UID: \"cd3d5358-af96-4d35-9091-75d0cb457bf2\") " pod="default/flask-test-app-7b6cd966fb-pxtxl"
Sep 17 04:54:15 minikube kubelet[2620]: I0917 04:54:15.132291    2620 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-r9lgh\" (UniqueName: \"kubernetes.io/projected/33bac6c2-1b7a-46e2-91f8-3275e0e48f81-kube-api-access-r9lgh\") pod \"flask-test-app-7b6cd966fb-fpwld\" (UID: \"33bac6c2-1b7a-46e2-91f8-3275e0e48f81\") " pod="default/flask-test-app-7b6cd966fb-fpwld"
Sep 17 04:54:15 minikube kubelet[2620]: I0917 04:54:15.132356    2620 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-j5lm4\" (UniqueName: \"kubernetes.io/projected/f1146fba-bba4-420c-b875-b07c25473838-kube-api-access-j5lm4\") pod \"flask-test-app-7b6cd966fb-fh8rb\" (UID: \"f1146fba-bba4-420c-b875-b07c25473838\") " pod="default/flask-test-app-7b6cd966fb-fh8rb"
Sep 17 04:54:15 minikube kubelet[2620]: I0917 04:54:15.132597    2620 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-zz2wd\" (UniqueName: \"kubernetes.io/projected/554aeab9-f74b-4f13-a41c-a12410a69da0-kube-api-access-zz2wd\") pod \"flask-test-app-7b6cd966fb-mgghv\" (UID: \"554aeab9-f74b-4f13-a41c-a12410a69da0\") " pod="default/flask-test-app-7b6cd966fb-mgghv"
Sep 17 04:54:15 minikube kubelet[2620]: I0917 04:54:15.233740    2620 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-vskrm\" (UniqueName: \"kubernetes.io/projected/b2bdd5f0-bc1d-4964-9983-78b453a0da80-kube-api-access-vskrm\") pod \"flask-test-app-7b6cd966fb-55nbq\" (UID: \"b2bdd5f0-bc1d-4964-9983-78b453a0da80\") " pod="default/flask-test-app-7b6cd966fb-55nbq"
Sep 17 04:54:15 minikube kubelet[2620]: I0917 04:54:15.234358    2620 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-hzggr\" (UniqueName: \"kubernetes.io/projected/1a22c12e-1a10-461d-abb5-6a5e378c3f30-kube-api-access-hzggr\") pod \"flask-test-app-7b6cd966fb-6vsql\" (UID: \"1a22c12e-1a10-461d-abb5-6a5e378c3f30\") " pod="default/flask-test-app-7b6cd966fb-6vsql"
Sep 17 04:54:15 minikube kubelet[2620]: I0917 04:54:15.234493    2620 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-c6t4w\" (UniqueName: \"kubernetes.io/projected/b03879f5-e05e-4043-acf2-e31fd8f3100b-kube-api-access-c6t4w\") pod \"flask-test-app-7b6cd966fb-dhc8x\" (UID: \"b03879f5-e05e-4043-acf2-e31fd8f3100b\") " pod="default/flask-test-app-7b6cd966fb-dhc8x"
Sep 17 04:54:15 minikube kubelet[2620]: I0917 04:54:15.234588    2620 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-dm4w4\" (UniqueName: \"kubernetes.io/projected/87005dee-2bdd-4884-86e2-e8eaae1fd4f0-kube-api-access-dm4w4\") pod \"flask-test-app-7b6cd966fb-87hv2\" (UID: \"87005dee-2bdd-4884-86e2-e8eaae1fd4f0\") " pod="default/flask-test-app-7b6cd966fb-87hv2"
Sep 17 04:54:15 minikube kubelet[2620]: I0917 04:54:15.305866    2620 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-r2sq8\" (UniqueName: \"kubernetes.io/projected/fafe4c4d-695f-43df-9fe5-3c0de0601ba4-kube-api-access-r2sq8\") pod \"flask-test-app-7b6cd966fb-nskhp\" (UID: \"fafe4c4d-695f-43df-9fe5-3c0de0601ba4\") " pod="default/flask-test-app-7b6cd966fb-nskhp"
Sep 17 04:54:15 minikube kubelet[2620]: I0917 04:54:15.306213    2620 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-n56fc\" (UniqueName: \"kubernetes.io/projected/d581a50e-c799-4d4a-aacd-cd12d8fea5b4-kube-api-access-n56fc\") pod \"flask-test-app-7b6cd966fb-g7h78\" (UID: \"d581a50e-c799-4d4a-aacd-cd12d8fea5b4\") " pod="default/flask-test-app-7b6cd966fb-g7h78"
Sep 17 04:54:15 minikube kubelet[2620]: I0917 04:54:15.307497    2620 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-qr4rr\" (UniqueName: \"kubernetes.io/projected/6c9b60ef-5f02-4611-8535-bc097eab20b6-kube-api-access-qr4rr\") pod \"flask-test-app-7b6cd966fb-445km\" (UID: \"6c9b60ef-5f02-4611-8535-bc097eab20b6\") " pod="default/flask-test-app-7b6cd966fb-445km"
Sep 17 04:54:15 minikube kubelet[2620]: I0917 04:54:15.308276    2620 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-qcbg5\" (UniqueName: \"kubernetes.io/projected/b9b82349-a100-4a41-aea6-4964a8d51827-kube-api-access-qcbg5\") pod \"flask-test-app-7b6cd966fb-pkl9v\" (UID: \"b9b82349-a100-4a41-aea6-4964a8d51827\") " pod="default/flask-test-app-7b6cd966fb-pkl9v"
Sep 17 04:54:15 minikube kubelet[2620]: I0917 04:54:15.308985    2620 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-kptmc\" (UniqueName: \"kubernetes.io/projected/c4f5837e-06d2-404d-be14-e02548aeffdb-kube-api-access-kptmc\") pod \"flask-test-app-7b6cd966fb-dk6xk\" (UID: \"c4f5837e-06d2-404d-be14-e02548aeffdb\") " pod="default/flask-test-app-7b6cd966fb-dk6xk"
Sep 17 04:54:15 minikube kubelet[2620]: I0917 04:54:15.311169    2620 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-v5r2q\" (UniqueName: \"kubernetes.io/projected/e96ad978-d379-43b7-9d43-5ce19b044d4c-kube-api-access-v5r2q\") pod \"flask-test-app-7b6cd966fb-lst8m\" (UID: \"e96ad978-d379-43b7-9d43-5ce19b044d4c\") " pod="default/flask-test-app-7b6cd966fb-lst8m"
Sep 17 04:54:15 minikube kubelet[2620]: I0917 04:54:15.311496    2620 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-g5q47\" (UniqueName: \"kubernetes.io/projected/085d54f5-eb4a-4321-bfec-f00f2c0dbbf9-kube-api-access-g5q47\") pod \"flask-test-app-7b6cd966fb-dv5z7\" (UID: \"085d54f5-eb4a-4321-bfec-f00f2c0dbbf9\") " pod="default/flask-test-app-7b6cd966fb-dv5z7"
Sep 17 04:54:15 minikube kubelet[2620]: I0917 04:54:15.311701    2620 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-jzf25\" (UniqueName: \"kubernetes.io/projected/34af2c95-b2dd-40c5-84e1-0bacc74fc3f3-kube-api-access-jzf25\") pod \"flask-test-app-7b6cd966fb-cdvmr\" (UID: \"34af2c95-b2dd-40c5-84e1-0bacc74fc3f3\") " pod="default/flask-test-app-7b6cd966fb-cdvmr"
Sep 17 04:54:15 minikube kubelet[2620]: I0917 04:54:15.313867    2620 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-wbljl\" (UniqueName: \"kubernetes.io/projected/4e3af1a3-b654-46b5-b661-38bd46d56089-kube-api-access-wbljl\") pod \"flask-test-app-7b6cd966fb-fdtsq\" (UID: \"4e3af1a3-b654-46b5-b661-38bd46d56089\") " pod="default/flask-test-app-7b6cd966fb-fdtsq"
Sep 17 04:54:15 minikube kubelet[2620]: I0917 04:54:15.313950    2620 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-9vp2j\" (UniqueName: \"kubernetes.io/projected/305e93ab-073a-4af2-9297-a293cd7ebe50-kube-api-access-9vp2j\") pod \"flask-test-app-7b6cd966fb-wdxcs\" (UID: \"305e93ab-073a-4af2-9297-a293cd7ebe50\") " pod="default/flask-test-app-7b6cd966fb-wdxcs"
Sep 17 04:54:15 minikube kubelet[2620]: I0917 04:54:15.420038    2620 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-gpsdg\" (UniqueName: \"kubernetes.io/projected/8c869875-954d-416d-825d-d585e53df838-kube-api-access-gpsdg\") pod \"flask-test-app-7b6cd966fb-wpd8s\" (UID: \"8c869875-954d-416d-825d-d585e53df838\") " pod="default/flask-test-app-7b6cd966fb-wpd8s"
Sep 17 04:54:32 minikube kubelet[2620]: I0917 04:54:32.815864    2620 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="c9aa65513ac664e00b416cf524bec2f8039e289d20d50f09d081f2529b49c475"
Sep 17 04:54:33 minikube kubelet[2620]: I0917 04:54:33.422071    2620 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="47674e10f8afd43f7f23baf58e75e3e2930d6bcba9bc21d22950cd31de0f2cff"
Sep 17 04:54:33 minikube kubelet[2620]: I0917 04:54:33.510876    2620 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="46fe4bfe2eb6a38b1f32a79da0106794e303e1c2f040d58aab221e5ab68f3db0"
Sep 17 04:54:33 minikube kubelet[2620]: I0917 04:54:33.523532    2620 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="47e9ba21cbcbd7ad9aea631282bc71afb1726e1a0be5899697cdede75c760bff"
Sep 17 04:54:33 minikube kubelet[2620]: I0917 04:54:33.605622    2620 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="813a626a6f1017bf6abcae75da38e909bc89f9fb898a52e75c233501e4fca244"
Sep 17 04:54:33 minikube kubelet[2620]: I0917 04:54:33.620339    2620 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="eb0164b2ebc288e44e606bbef81c7a02dfff6a0725761e54354e145b374fccf3"
Sep 17 04:54:33 minikube kubelet[2620]: I0917 04:54:33.631151    2620 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="7910540e2c0b7b1172ad3a89be3ac928b56a1afc4ef3d3cfd56ca370141061f5"


==> storage-provisioner [e3626bb0547e] <==
I0917 04:49:57.593461       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0917 04:49:57.626478       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0917 04:49:57.628329       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0917 04:49:57.649738       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0917 04:49:57.650874       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_63de3ab8-b5f5-406d-8be2-1c98a905980d!
I0917 04:49:57.650918       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"83a8169a-8d2b-40fd-b152-b750577d1254", APIVersion:"v1", ResourceVersion:"442", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_63de3ab8-b5f5-406d-8be2-1c98a905980d became leader
I0917 04:49:57.753413       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_63de3ab8-b5f5-406d-8be2-1c98a905980d!


==> storage-provisioner [fec7e1c1e5f5] <==
I0917 04:49:40.271484       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0917 04:49:40.327311       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": x509: certificate signed by unknown authority

